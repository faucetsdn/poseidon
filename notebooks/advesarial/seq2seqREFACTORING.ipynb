{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how data are represented at each level (forward, backward, forward with padding on top) needs a little\n",
    "    #experimentation to determine the best representation\n",
    "    #also, is encoding at each layer really the best way? or just feeding the raw through?\n",
    "    \n",
    "#Outside web ips are going to be a problem/messy/noisy. Start by categorizing all outside ips by <OUTSIDE_IP>\n",
    "    #instead of the ip address, or another 4 digit symbol to insert into the hex string.\n",
    "    \n",
    "#to help the models generalize more, for a given source ip address with probability p (say p = 0.1) \n",
    "    #use the token <OTHER_MACHINE>\n",
    "    \n",
    "#should we remove random parts of the header, i.e. checksum\n",
    "\n",
    "#should I take out bias for RNNs?\n",
    "\n",
    "#for the decoder,does the fork encoding need to happen ?\n",
    "    #do we simply cat the hContext with the next words?\n",
    "    \n",
    "#Should the architecture just be encode, context and then prediction???\n",
    "\n",
    "#Input data, should it have character and hex pair encoding as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu'\n",
    "\n",
    "import ast\n",
    "import json\n",
    "import subprocess\n",
    "import cPickle\n",
    "import sys\n",
    "import binascii\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP, \\\n",
    "                                Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "sys.setrecursionlimit(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxPackets = 2\n",
    "packetTimeSteps = 80\n",
    "loadPrepedData = True\n",
    "dataPath = '/data/fs4/home/bradh/bigFlows.pickle'\n",
    "\n",
    "packetReverse = False\n",
    "padOldTimeSteps = True\n",
    "\n",
    "runname = 'hredClassify2smallpackets'\n",
    "rnnType = 'gru' #gru or lstm\n",
    "\n",
    "wtstd = 0.2\n",
    "dimIn = 257 #hex has 256 characters + the <EOP> character\n",
    "dim = 100 #dimension reduction size\n",
    "batch_size = 20\n",
    "numClasses = 4\n",
    "clippings = 1\n",
    "\n",
    "epochs = 50\n",
    "lr = 0.0001\n",
    "decay = 0.9\n",
    "trainPercent = 0.9\n",
    "\n",
    "module_logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def parse_header(line):\n",
    "    ret_dict = {}\n",
    "    h = line.split()\n",
    "    #ret_dict['direction'] = \" \".join(h[3:6])\n",
    "    if h[2] == 'IP6':\n",
    "        \"\"\"\n",
    "        Conditional formatting based on ethernet type.\n",
    "        IPv4 format: 0.0.0.0.port\n",
    "        IPv6 format (one of many): 0:0:0:0:0:0.port\n",
    "        \"\"\"\n",
    "        ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "        ret_dict['src_ip'] = h[3].split('.')[0]\n",
    "        ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "        ret_dict['dest_ip'] = h[5].split('.')[0]\n",
    "    else:\n",
    "        if len(h[3].split('.')) > 4:\n",
    "            ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "            ret_dict['src_ip'] = '.'.join(h[3].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['src_ip'] = h[3]\n",
    "            ret_dict['src_port'] = ''\n",
    "        if len(h[5].split('.')) > 4:\n",
    "            ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "            ret_dict['dest_ip'] = '.'.join(h[5].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['dest_ip'] = h[5].split(':')[0]\n",
    "            ret_dict['dest_port'] = ''\n",
    "    return ret_dict\n",
    "\n",
    "def parse_data(line):\n",
    "    ret_str = ''\n",
    "    h, d = line.split(':', 1)\n",
    "    ret_str = d.strip().replace(' ', '')\n",
    "    return ret_str\n",
    "\n",
    "def process_packet(output):\n",
    "    # TODO!! throws away the first packet!\n",
    "    ret_header = {}\n",
    "    ret_dict = {}\n",
    "    ret_data = ''\n",
    "    hasHeader = False\n",
    "    for line in output:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if not line.startswith('0x'):\n",
    "                # header line\n",
    "                if ret_dict and ret_data:\n",
    "                    # about to start new header, finished with hex\n",
    "                    ret_dict['data'] = ret_data\n",
    "                    yield ret_dict\n",
    "                    ret_dict.clear()\n",
    "                    ret_header.clear()\n",
    "                    ret_data = ''\n",
    "                    hasHeader = False\n",
    "                    \n",
    "                # parse next header    \n",
    "                try:\n",
    "                    ret_header = parse_header(line)\n",
    "                    ret_dict.update(ret_header)\n",
    "                    hasHeader = True\n",
    "                except:\n",
    "                    ret_header.clear()\n",
    "                    ret_dict.clear()\n",
    "                    ret_data = ''\n",
    "                    hasHeader = False\n",
    "                    \n",
    "            else:\n",
    "                # hex data line\n",
    "                if hasHeader:\n",
    "                    data = parse_data(line)\n",
    "                    ret_data = ret_data + data\n",
    "                else:\n",
    "                    continue\n",
    "    \n",
    "def is_clean_packet(packet):\n",
    "    \"\"\"\n",
    "    Returns whether or not the parsed packet is valid\n",
    "    or not. Checks that both the src and dest\n",
    "    ports are integers. Checks that src and dest IPs\n",
    "    are valid address formats. Checks that packet data\n",
    "    is hex. Returns True if all tests pass, False otherwise.\n",
    "    \"\"\"\n",
    "    if not packet['src_port'].isdigit(): return False\n",
    "    if not packet['dest_port'].isdigit(): return False\n",
    "    \n",
    "    if packet['src_ip'].isalpha(): return False\n",
    "    if packet['dest_ip'].isalpha(): return False\n",
    "    #try:\n",
    "    #    ipaddress.ip_address(packet['src_ip'])\n",
    "    #    ipaddress.ip_address(packet['dest_ip'])\n",
    "    #except:\n",
    "    #    return False\n",
    "     \n",
    "    if 'data' in packet:\n",
    "        try:\n",
    "            int(packet['data'], 16)\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "def order_keys(hexSessionDict):\n",
    "    orderedKeys = []\n",
    "    \n",
    "    for key in sorted(hexSessionDict.keys(), key=lambda key: hexSessionDict[key][1]):\n",
    "        orderedKeys.append(key) \n",
    "        \n",
    "    return orderedKeys\n",
    "\n",
    "def read_pcap(path):\n",
    "    hex_sessions = {}\n",
    "    proc = subprocess.Popen('tcpdump -nn -tttt -xx -r '+path,\n",
    "                            shell=True,\n",
    "                            stdout=subprocess.PIPE)\n",
    "    insert_num = 0  # keeps track of insertion order into dict\n",
    "    for packet in process_packet(proc.stdout):\n",
    "        if not is_clean_packet(packet):\n",
    "            continue\n",
    "        if 'data' in packet:\n",
    "            key = (packet['src_ip']+\":\"+packet['src_port'], packet['dest_ip']+\":\"+packet['dest_port'])\n",
    "            rev_key = (key[1], key[0])\n",
    "            if key in hex_sessions:\n",
    "                hex_sessions[key][0].append(packet['data'])\n",
    "            elif rev_key in hex_sessions:\n",
    "                hex_sessions[rev_key][0].append(packet['data'])\n",
    "            else:\n",
    "                hex_sessions[key] = ([packet['data']], insert_num)\n",
    "                insert_num += 1\n",
    "        \n",
    "    return hex_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickleFile(thing2save, file2save2 = None, filePath='/work/notebooks/drawModels/', fileName = 'myModels'):\n",
    "    \n",
    "    if file2save2 == None:\n",
    "        f=file(filePath+fileName+'.pickle', 'wb')\n",
    "    else:\n",
    "        f=file(filePath+file2save2, 'wb')\n",
    "        \n",
    "    cPickle.dump(thing2save, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "def loadFile(filePath):\n",
    "    file2open = file(filePath, 'rb')\n",
    "    loadedFile = cPickle.load(file2open)\n",
    "    file2open.close()\n",
    "    \n",
    "    return loadedFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeBadSessionizer(hexSessionDict, saveFile=False, dataPath=None, fileName=None):\n",
    "    for ses in hexSessionDict.keys():\n",
    "        paclens = []\n",
    "        for pac in hexSessionDict[ses][0]:\n",
    "            paclens.append(len(pac))\n",
    "        if np.min(paclens)<80:\n",
    "            del hexSessionDict[ses]\n",
    "\n",
    "    if saveFile:\n",
    "        print 'pickling sessions'\n",
    "        pickleFile(hexSessionDict, filePath=dataPath, fileName=fileName)\n",
    "        \n",
    "    return hexSessionDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "def hexTokenizer():\n",
    "    hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    "    ,\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    "    ,\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    "    ,\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    "    ,\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    "    ,\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    "    ,\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    "    ,\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    "    ,\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    "    ,\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "    hexList = [x.strip() for x in hexstring.lower().split(',')]\n",
    "    hexList.append('<EOP>') #End Of Packet token\n",
    "    #EOS token??????\n",
    "    hexDict = {}\n",
    "\n",
    "    for key, val in enumerate(hexList):\n",
    "        if len(val) == 1:\n",
    "            val = '0'+val\n",
    "        hexDict[val] = key  #dictionary: k=hex, v=int  \n",
    "    \n",
    "    return hexDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary of IP communications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def srcIpDict(hexSessionDict):\n",
    "    ''' \n",
    "    input: dictionary of key = sessions, value = list of HEX HEADERS of packets in session\n",
    "    output: dictionary of key = source IP, value/subkey = dictionary of destination IPs, \n",
    "                                           subvalue = [[sport], [dport], [plen], [protocol]]\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    srcIpDict = OrderedDict()   \n",
    "    uniqIPs = [] #some ips are dest only. this will collect all ips, not just srcIpDict.keys()\n",
    "    \n",
    "    for session in hexSessionDict.keys():\n",
    "        \n",
    "        for rawpacket in hexSessionDict[session][0]:\n",
    "            packet = copy(rawpacket)\n",
    "            \n",
    "            dstIpSubDict = {}\n",
    "            \n",
    "            sourceMAC = packet[:12]\n",
    "            destMAC = packet[12:24]\n",
    "            srcip = packet[52:60]\n",
    "            dstip = packet[60:68]\n",
    "            sport = packet[68:72]\n",
    "            dport = packet[72:76]\n",
    "            plen = packet[32:36]\n",
    "            protocol = packet[46:48]\n",
    "            \n",
    "            uniqIPs = list(set(uniqIPs) | set([dstip, srcip]))\n",
    "\n",
    "            if srcip not in srcIpDict:\n",
    "                dstIpSubDict[dstip] = [[sport], [dport], [plen], [protocol], [sourceMAC], [destMAC]]\n",
    "                srcIpDict[srcip] = dstIpSubDict\n",
    "\n",
    "            if dstip not in srcIpDict[srcip]:    \n",
    "                srcIpDict[srcip][dstip] = [[sport], [dport], [plen], [protocol], [sourceMAC], [destMAC]]\n",
    "            else:\n",
    "                srcIpDict[srcip][dstip][0].append(sport)\n",
    "                srcIpDict[srcip][dstip][1].append(dport)\n",
    "                srcIpDict[srcip][dstip][2].append(plen)\n",
    "                srcIpDict[srcip][dstip][3].append(protocol)\n",
    "                srcIpDict[srcip][dstip][4].append(sourceMAC)\n",
    "                srcIpDict[srcip][dstip][5].append(destMAC)\n",
    "                \n",
    "    return srcIpDict, uniqIPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dictUniquerizer(dictOdictsOlistOlists):\n",
    "    '''\n",
    "    input: dictionary of dictionaries that have a list of lists \n",
    "           ex. srcIpDict[srcip][dstip] = [[sport], [dport], [plen], [protocol]]\n",
    "    output: dictionary of dictionaries with list of lists with unique items in the final sublist\n",
    "    \n",
    "    WARNING: will overwrite your input dictionary. Make a copy if you want to preserve dictOdictsOlistOlists.\n",
    "    '''\n",
    "    #dictCopy\n",
    "    for key in dictOdictsOlistOlists.keys():\n",
    "        for subkey in dictOdictsOlistOlists[key].keys():\n",
    "            for sublist in xrange(len(dictOdictsOlistOlists[key][subkey])):\n",
    "                dictOdictsOlistOlists[key][subkey][sublist] = list(set(dictOdictsOlistOlists[key][subkey][sublist]))\n",
    "    \n",
    "    return dictOdictsOlistOlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: parallelize\n",
    "print 'creating dictionary of ip communications'\n",
    "\n",
    "#comsDicttest, uniqIPstest = srcIpDict(hexSessionstest)\n",
    "comsDict, uniqIPs = srcIpDict(hexSessions)\n",
    "#test\n",
    "comsDict = dictUniquerizer(comsDict)\n",
    "#comsDicttest = dictUniquerizer(comsDicttest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directionality adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ipDirSwitcher(hexSessionList):\n",
    "    '''\n",
    "    switches both ip and mac addresses\n",
    "    input is a list of packets from ONE session\n",
    "    '''\n",
    "    \n",
    "    ipdirsession = []\n",
    "        \n",
    "    for p in hexSessionList:\n",
    "        sourceMAC = p[:12]\n",
    "        destMAC = p[12:24]\n",
    "        sourceIP = p[52:60]\n",
    "        destIP = p[60:68]\n",
    "\n",
    "        ipdirsession.append(destMAC + sourceMAC + p[24:52] + destIP + sourceIP + p[68:])\n",
    "\n",
    "    return ipdirsession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def portDirSwitcher(hexSessionList):\n",
    "    '''\n",
    "    input is a list of packets from ONE session\n",
    "    '''\n",
    "    \n",
    "    portdirsession = []\n",
    "    \n",
    "    for p in hexSessionList:\n",
    "        sport = p[68:72]\n",
    "        dport = p[72:76]\n",
    "\n",
    "        portdirsession.append(p[:68]+dport+sport+p[76:])\n",
    "\n",
    "    return portdirsession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave one swap one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: fix the mac switchout\n",
    "def dstIpSwapOut(hexSessionList, dictOcoms, listOuniqIPs):\n",
    "    #srcIpDict[srcip][dstip] = [[sport], [dport], [plen], [protocol]]\n",
    "    \n",
    "    swapSession = []\n",
    "    sourceMAC = hexSessionList[0][:12]#[0] assumes first packet contains true initial direction\n",
    "    destMAC = hexSessionList[0][12:24]\n",
    "    srcip = hexSessionList[0][52:60] \n",
    "    dstip = hexSessionList[0][60:68]\n",
    "    normDstIps = dictOcoms[srcip].keys()+[srcip] #get list of dstIPs that srcIP talks to\n",
    "    abbynormIps = copy(listOuniqIPs)\n",
    "    \n",
    "    for normIp in normDstIps:\n",
    "        abbynormIps.remove(normIp) #remove itself and know dstIPs from list of consideration.\n",
    "    \n",
    "    abbynormDestIp = random.sample(abbynormIps, 1)[0] #get random ip that srcip doesn't talk to\n",
    "\n",
    "    for rawpacket in hexSessionList:\n",
    "        packet = copy(rawpacket)\n",
    "        \n",
    "        if packet[60:68] == dstip:\n",
    "            packet = packet[:60] + abbynormDestIp + packet[68:] #\n",
    "        elif packet[60:68] == srcip:\n",
    "            packet = packet[:52] + abbynormDestIp + packet[60:] #in case direction switches for packet in session\n",
    "            \n",
    "        swapSession.append(packet)\n",
    "\n",
    "    return swapSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def dstPortSwapOneOut(hexSessionList):\n",
    "    #THINK THROUGH    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oneSessionEncoder(sessionPackets, hexDict, maxPackets = 2, packetTimeSteps = 100,\n",
    "                       packetReverse = False, charLevel = False, padOldTimeSteps = True):    \n",
    "            \n",
    "    sessionCollect = []\n",
    "    packetCollect = []\n",
    "    \n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    if len(sessionPackets) > maxPackets: #crop the number of sessions to maxPackets\n",
    "        sessionList = copy(sessionPackets[:maxPackets])\n",
    "    else:\n",
    "        sessionList = copy(sessionPackets)\n",
    "\n",
    "    for rawpacket in sessionList:\n",
    "        packet = copy(rawpacket)\n",
    "        packet = packet[32:36]+packet[44:46]+packet[46:48]+packet[52:60]+packet[60:68]+\\\n",
    "                 packet[68:70]+packet[70:72]+packet[72:74]\n",
    "        packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "            \n",
    "        if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "            packet = packet[:packetTimeSteps]\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        else:\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        \n",
    "        packetCollect.append(packet)\n",
    "        \n",
    "        pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "        pacMatLen = len(pacMat)\n",
    "        \n",
    "        #padding packet\n",
    "        if packetReverse:\n",
    "            pacMat = pacMat[::-1]\n",
    "\n",
    "        if pacMatLen < packetTimeSteps:\n",
    "            #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "            #padding the packet such that zeros are after the actual info for better translation\n",
    "            if padOldTimeSteps:\n",
    "                pacMat = np.vstack( ( np.zeros((packetTimeSteps-pacMatLen,vecLen)), pacMat) ) \n",
    "            else:\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "        if pacMatLen > packetTimeSteps:\n",
    "            pacMat = pacMat[:packetTimeSteps, :]\n",
    "\n",
    "        sessionCollect.append(pacMat)\n",
    "\n",
    "    #padding session\n",
    "    sessionCollect = np.asarray(sessionCollect, dtype=theano.config.floatX)\n",
    "    numPacketsInSession = sessionCollect.shape[0]\n",
    "    if numPacketsInSession < maxPackets:\n",
    "        #pad sessions to fit the \n",
    "        sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession, \n",
    "                                                             packetTimeSteps, vecLen))) )\n",
    "    \n",
    "    return sessionCollect, packetCollect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    \n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictClass(predictFun, hexSessionsDict, comsDict, uniqIPs, hexDict, hexSessionsKeys,\n",
    "                 numClasses = 4, trainPercent = 0.9, dimIn=257, maxPackets=2,\n",
    "                 packetTimeSteps = 16, padOldTimeSteps=True):\n",
    "    \n",
    "    testCollect = []\n",
    "    predtargets = []\n",
    "    actualtargets = []\n",
    "    trainPercent = 0.9\n",
    "    trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "        \n",
    "    start = trainIndex\n",
    "    end = len(hexSessionsKeys)\n",
    "        \n",
    "    trainingSessions = []\n",
    "    trainingTargets = []\n",
    "\n",
    "    for trainKey in range(start, end):\n",
    "        sessionForEncoding = list(hexSessionsDict[hexSessionsKeys[trainKey]][0])\n",
    "\n",
    "        adversaryList = [sessionForEncoding, \n",
    "                         dstIpSwapOut(sessionForEncoding, comsDict, uniqIPs), \n",
    "                         portDirSwitcher(sessionForEncoding), \n",
    "                         ipDirSwitcher(sessionForEncoding)]\n",
    "        abbyIndex = random.sample(range(len(adversaryList)), 1)[0]\n",
    "        abbyOneHotSes = oneSessionEncoder(adversaryList[abbyIndex],\n",
    "                                          hexDict = hexDict,\n",
    "                                          packetReverse=packetReverse, \n",
    "                                          padOldTimeSteps = padOldTimeSteps, \n",
    "                                          maxPackets = maxPackets, \n",
    "                                          packetTimeSteps = packetTimeSteps)\n",
    "\n",
    "        targetClasses = [0]*numClasses\n",
    "        targetClasses[abbyIndex] = 1\n",
    "        abbyTarget = np.array(targetClasses, dtype=theano.config.floatX)\n",
    "        trainingSessions.append(abbyOneHotSes[0])\n",
    "        trainingTargets.append(abbyTarget)\n",
    "\n",
    "    sessionsMinibatch = np.asarray(trainingSessions, dtype=theano.config.floatX)\\\n",
    "                                   .reshape((-1, packetTimeSteps, 1, dimIn))\n",
    "    targetsMinibatch = np.asarray(trainingTargets, dtype=theano.config.floatX)\n",
    "\n",
    "    predcostfun = predictFun(sessionsMinibatch)\n",
    "    testCollect.append(np.mean(np.argmax(predcostfun,axis=1) == np.argmax(targetsMinibatch, axis=1)))\n",
    "\n",
    "    predtargets = np.argmax(predcostfun,axis=1)\n",
    "    actualtargets = np.argmax(targetsMinibatch, axis=1)\n",
    "\n",
    "    print \"TEST accuracy:         \", np.mean(testCollect)\n",
    "    print\n",
    "\n",
    "    return predtargets, actualtargets, np.mean(testCollect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binaryPrecisionRecall(predictions, targets, numClasses = 4):\n",
    "    for cla in range(numClasses):\n",
    "        \n",
    "        confustop = np.array([])\n",
    "        confusbottom = np.array([])\n",
    "\n",
    "        predictions = np.asarray(predictions).flatten()\n",
    "        targets = np.asarray(targets).flatten()\n",
    "\n",
    "        pred1 = np.where(predictions == cla)\n",
    "        pred0 = np.where(predictions != cla)\n",
    "        target1 = np.where(targets == cla)\n",
    "        target0 = np.where(targets != cla)\n",
    "\n",
    "        truePos = np.intersect1d(pred1[0],target1[0]).shape[0]\n",
    "        trueNeg = np.intersect1d(pred0[0],target0[0]).shape[0]\n",
    "        falsePos = np.intersect1d(pred1[0],target0[0]).shape[0]\n",
    "        falseNeg = np.intersect1d(pred0[0],target1[0]).shape[0]\n",
    "\n",
    "        top = np.append(confustop, (truePos, falsePos))\n",
    "        bottom = np.append(confusbottom, (falseNeg, trueNeg))\n",
    "        confusionMatrix = np.vstack((top, bottom))\n",
    "        \n",
    "        precision  = float(truePos)/(truePos + falsePos + 0.00001) #1 - (how much junk did we give user)\n",
    "        recall = float(truePos)/(truePos + falseNeg + 0.00001) #1 - (how much good stuff did we miss)\n",
    "        f1 = 2*((precision*recall)/(precision+recall+0.00001))\n",
    "        \n",
    "        print 'class '+str(cla)+' precision: ', precision\n",
    "        print 'class '+str(cla)+' recall:    ', recall\n",
    "        print 'class '+str(cla)+' f1:        ', f1\n",
    "        print\n",
    "    \n",
    "        #return confusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization for both the unsupervised net and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training(runname, rnnType, maxPackets, packetTimeSteps, packetReverse, padOldTimeSteps, wtstd, \n",
    "             lr, decay, clippings, dimIn, dim, numClasses, batch_size, epochs, \n",
    "             trainPercent, dataPath, loadPrepedData = False):\n",
    "    print locals()\n",
    "    print\n",
    "    \n",
    "    X = T.tensor4('inputs')\n",
    "    Y = T.matrix('targets')\n",
    "    linewt_init = IsotropicGaussian(wtstd)\n",
    "    line_bias = Constant(1.0)\n",
    "    rnnwt_init = IsotropicGaussian(wtstd)\n",
    "    rnnbias_init = Constant(0.0)\n",
    "    classifierWts = IsotropicGaussian(wtstd)\n",
    "\n",
    "    learning_rateClass = theano.shared(np.array(lr, dtype=theano.config.floatX))\n",
    "    learning_decay = np.array(decay, dtype=theano.config.floatX)\n",
    "    \n",
    "    ###DATA PREP\n",
    "    print 'loading data'\n",
    "    if loadPrepedData:\n",
    "        hexSessions = loadFile(dataPath)\n",
    "\n",
    "    else:\n",
    "        hexSessions = read_pcap(dataPath)\n",
    "        hexSessions = removeBadSessionizer(hexSessions)\n",
    "\n",
    "    hexSessionsKeys = order_keys(hexSessions)\n",
    "    hexDict = hexTokenizer()\n",
    "    \n",
    "    print 'creating dictionary of ip communications'\n",
    "    comsDict, uniqIPs = srcIpDict(hexSessions)\n",
    "    comsDict = dictUniquerizer(comsDict)\n",
    "     \n",
    "    print 'initializing network graph'\n",
    "    ###ENCODER\n",
    "    if rnnType == 'gru':\n",
    "        rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "        dimMultiplier = 2\n",
    "    else:\n",
    "        rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "        dimMultiplier = 4\n",
    "\n",
    "    fork = Fork(output_names=['linear', 'gates'],\n",
    "                name='fork', input_dim=dimIn, output_dims=[dim, dim * dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "    ###CONTEXT\n",
    "    if rnnType == 'gru':\n",
    "        rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                    biases_init = rnnbias_init, name = 'gruContext')\n",
    "    else:\n",
    "        rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                          name = 'lstmContext')\n",
    "\n",
    "    forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "                name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "    forkDec = Fork(output_names=['linear', 'gates'],\n",
    "                name='forkDec', input_dim=dim, output_dims=[dim, dim*dimMultiplier], \n",
    "                weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "    #CLASSIFIER\n",
    "    bmlp = BatchNormalizedMLP( activations=[Logistic(),Logistic()], \n",
    "               dims=[dim, dim, numClasses],\n",
    "               weights_init=classifierWts,\n",
    "               biases_init=Constant(0.0001) )\n",
    "\n",
    "    #initialize the weights in all the functions\n",
    "    fork.initialize()\n",
    "    rnn.initialize()\n",
    "    forkContext.initialize()\n",
    "    rnnContext.initialize()\n",
    "    forkDec.initialize()\n",
    "    bmlp.initialize()\n",
    "\n",
    "    def onestepEnc(X):\n",
    "        data1, data2 = fork.apply(X) \n",
    "\n",
    "        if rnnType == 'gru':\n",
    "            hEnc = rnn.apply(data1, data2) \n",
    "        else:\n",
    "            hEnc, _ = rnn.apply(data2)\n",
    "\n",
    "        return hEnc\n",
    "\n",
    "    hEnc, _ = theano.scan(onestepEnc, X) #(mini*numPackets, packetLen, 1, hexdictLen)\n",
    "    hEncReshape = T.reshape(hEnc[:,-1], (-1, maxPackets, 1, dim)) #[:,-1] takes the last rep for each packet\n",
    "                                                                 #(mini, numPackets, 1, dimReduced)\n",
    "    def onestepContext(hEncReshape):\n",
    "\n",
    "        data3, data4 = forkContext.apply(hEncReshape)\n",
    "\n",
    "        if rnnType == 'gru':\n",
    "            hContext = rnnContext.apply(data3, data4)\n",
    "        else:\n",
    "            hContext, _ = rnnContext.apply(data4)\n",
    "\n",
    "        return hContext\n",
    "\n",
    "    hContext, _ = theano.scan(onestepContext, hEncReshape)\n",
    "    hContextReshape = T.reshape(hContext[:,-1], (-1,dim))\n",
    "\n",
    "    data5, _ = forkDec.apply(hContextReshape)\n",
    "\n",
    "    pyx = bmlp.apply(data5)\n",
    "    softmax = Softmax()\n",
    "    softoutClass = softmax.apply(pyx)\n",
    "    costClass = T.mean(CategoricalCrossEntropy().apply(Y, softoutClass))\n",
    "\n",
    "    #CREATE GRAPH\n",
    "    cgClass = ComputationGraph([costClass])\n",
    "    paramsClass = VariableFilter(roles = [PARAMETER])(cgClass.variables)\n",
    "    updatesClass = Adam(paramsClass, costClass, learning_rateClass, c=clippings) \n",
    "    #updatesClass = RMSprop(costClass, paramsClass, learning_rateClass, c=clippings)\n",
    "\n",
    "    #print 'grad compiling'\n",
    "    #gradients = T.grad(costClass, paramsClass)\n",
    "    #gradients = clip_norms(gradients, clippings)\n",
    "    #gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "    #print 'finish with grads'\n",
    "\n",
    "    print 'compiling graph you talented soul'\n",
    "    classifierTrain = theano.function([X,Y], [costClass, hEnc, hContext, pyx, softoutClass], \n",
    "                                      updates=updatesClass, allow_input_downcast=True)\n",
    "    classifierPredict = theano.function([X], softoutClass, allow_input_downcast=True)\n",
    "    print 'finished compiling'\n",
    "\n",
    "    trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "\n",
    "    epochCost = []\n",
    "    gradNorms = []\n",
    "    trainAcc = []\n",
    "    testAcc = []\n",
    "\n",
    "    costCollect = []\n",
    "    trainCollect = []\n",
    "\n",
    "    print 'training begins'\n",
    "    iteration = 0\n",
    "    #epoch\n",
    "    for epoch in xrange(epochs):\n",
    "\n",
    "        #iteration/minibatch\n",
    "        for start, end in zip(range(0, trainIndex,batch_size),\n",
    "                              range(batch_size, trainIndex, batch_size)):\n",
    "\n",
    "            trainingTargets = []\n",
    "            trainingSessions = []\n",
    "\n",
    "            #create one minibatch with 0.5 normal and 0.5 abby normal traffic\n",
    "            for trainKey in range(start, end):\n",
    "                sessionForEncoding = list(hexSessions[hexSessions.keys()[trainKey]][0])\n",
    "\n",
    "                adversaryList = [sessionForEncoding, \n",
    "                                 dstIpSwapOut(sessionForEncoding, comsDict, uniqIPs), \n",
    "                                 portDirSwitcher(sessionForEncoding), \n",
    "                                 ipDirSwitcher(sessionForEncoding)]\n",
    "                abbyIndex = random.sample(range(len(adversaryList)), 1)[0]\n",
    "                \n",
    "                abbyOneHotSes = oneSessionEncoder(adversaryList[abbyIndex],\n",
    "                                                  hexDict = hexDict,\n",
    "                                                  packetReverse=packetReverse, \n",
    "                                                  padOldTimeSteps = padOldTimeSteps, \n",
    "                                                  maxPackets = maxPackets, \n",
    "                                                  packetTimeSteps = packetTimeSteps)\n",
    "\n",
    "                targetClasses = [0]*numClasses\n",
    "                targetClasses[abbyIndex] = 1\n",
    "                abbyTarget = np.array(targetClasses, dtype=theano.config.floatX)\n",
    "                trainingSessions.append(abbyOneHotSes[0])\n",
    "                trainingTargets.append(abbyTarget)\n",
    "\n",
    "            sessionsMinibatch = np.asarray(trainingSessions).reshape((-1, packetTimeSteps, 1, dimIn))\n",
    "            targetsMinibatch = np.asarray(trainingTargets)\n",
    "\n",
    "            costfun = classifierTrain(sessionsMinibatch, targetsMinibatch)\n",
    "\n",
    "            costCollect.append(costfun[0])\n",
    "            trainCollect.append(np.mean(np.argmax(costfun[-1],axis=1) == np.argmax(targetsMinibatch, axis=1)))\n",
    "\n",
    "            iteration+=1\n",
    "\n",
    "            if iteration == 1:\n",
    "                print 'you are amazing'\n",
    "\n",
    "\n",
    "            if iteration%200 == 0:\n",
    "                print\n",
    "                print '   Iteration: ', iteration\n",
    "                print '   Cost: ', np.mean(costCollect[-20:])\n",
    "                print '   TRAIN accuracy: ', np.mean(trainCollect[-20:])\n",
    "                print\n",
    "                \n",
    "                #grads = gradientFun(sessionsMinibatch, targetsMinibatch)\n",
    "                #for gra in grads:\n",
    "                #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "                \n",
    "                np.savetxt('/data/fs4/home/bradh/outputs/'+runname+\"_TRAIN.csv\", trainCollect[::50], delimiter=\",\")\n",
    "                np.savetxt('/data/fs4/home/bradh/outputs/'+runname+\"_COST.csv\", costCollect[::50], delimiter=\",\")\n",
    "\n",
    "            #testing accuracy\n",
    "            if iteration%500 == 0:\n",
    "                predtar, acttar, testCollect = predictClass(classifierPredict, hexSessions, comsDict, uniqIPs, hexDict,\n",
    "                                                            hexSessionsKeys,\n",
    "                                                            numClasses, trainPercent, dimIn, maxPackets, packetTimeSteps,\n",
    "                                                         padOldTimeSteps)\n",
    "\n",
    "                binaryPrecisionRecall(predtar, acttar)\n",
    "\n",
    "                testAcc.append(testCollect)\n",
    "                np.savetxt('/data/fs4/home/bradh/outputs/'+runname+\"_TEST.csv\", testAcc, delimiter=\",\")\n",
    "\n",
    "            #save the models\n",
    "            if iteration%1500 == 0:\n",
    "                pickleFile(classifierTrain, filePath='/data/fs4/home/bradh/outputs/',\n",
    "                            fileName=runname+'TRAIN'+str(iteration))\n",
    "                pickleFile(classifierPredict, filePath='/data/fs4/home/bradh/outputs/',\n",
    "                            fileName=runname+'PREDICT'+str(iteration))\n",
    "\n",
    "        epochCost.append(np.mean(costCollect[-50:]))\n",
    "        trainAcc.append(np.mean(trainCollect[-50:]))\n",
    "        \n",
    "        print 'Epoch: ', epoch\n",
    "        #module_logger.debug('Epoch:%r',epoch)\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        print 'Epoch TRAIN accuracy: ', trainAcc[-1]\n",
    "    \n",
    "    return classifierTrain, classifierPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TODO: expose classifier dim\n",
    "train, predict = training(wtstd, lr, decay, dim, dimIn, numClasses, batch_size, \n",
    "                          maxPackets, trainPercent, dataPath='bigFlows.pickle', loadPrepedData = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
