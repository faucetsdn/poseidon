{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import binascii\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "from scapy.all import *\n",
    "sys.path.append('hed-dlg/')\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy.stats import itemfreq\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "###These warnings do not impede progress\n",
    "#WARNING: Failed to execute tcpdump. Check it is installed and in the PATH\n",
    "#WARNING: No route found for IPv6 destination :: (no default route?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/bradsfirstpcaps.pcap'\n",
    "pcaps = rdpcap(dataPath)\n",
    "sessionPrep = pcaps.sessions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ipComs(listOsessions, hexOut = False):\n",
    "    '''\n",
    "    takes scapy sessions\n",
    "    \n",
    "    returns a dictionary of source ips and the ips they talk to, and a list of \n",
    "    all of the unique ip addresses in the data\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    ipAddressDict = {}\n",
    "    uniqIPs = []\n",
    "    \n",
    "    for k,v in listOsessions.items():\n",
    "        for p in v:\n",
    "            sourceIP = p.payload.fields['src']\n",
    "            destIP = p.payload.fields['dst']\n",
    "            \n",
    "            #if source ip is not in dictionary, then add it with dest ip as list\n",
    "            if sourceIP not in ipAddressDict:\n",
    "                ipAddressDict[sourceIP] = [destIP]\n",
    "            \n",
    "            else:\n",
    "                ipAddressDict[sourceIP] = list(set(ipAddressDict[sourceIP]) | set([destIP]))\n",
    "            \n",
    "            uniqIPs = list(set(uniqIPs) | set([destIP, sourceIP]))\n",
    "            \n",
    "    return ipAddressDict, uniqIPs\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sessionDict, uniqips = ipComs(sessionPrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ipDirectionSwitcher(hexSessionsDict):\n",
    "    badSessions = {}\n",
    "    \n",
    "    for k in hexSessionsDict.keys():\n",
    "        session = []\n",
    "        \n",
    "        for p in hexSessionsDict[k]:\n",
    "            sourceIP = p[52:60]\n",
    "            destIP = p[60:68]\n",
    "            \n",
    "            session.append(p[:52]+sourceIP+destIP+p[68:])\n",
    "        \n",
    "        badSessions[k] = session\n",
    "            \n",
    "    return badSessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "badSessionDict = ipDirectionSwitcher(hexSessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['90b11c2583303c15c2d18eb6080045000040e00b400040063fe80a30511c6869570ffc520050caafea0500000000b002ffff401f0000020405b4010303050101080a4330e7900000000004020000',\n",
       " '90b11c2583303c15c2d18eb608004500003493d3400040068c2c0a30511c6869570ffc520050caafea06e0c8848e8010100ce4fc00000101080a4330e7ecbf086619',\n",
       " '90b11c2583303c15c2d18eb60800450001090e5d4000400610ce0a30511c6869570ffc520050caafea06e0c8848e8018100c6eb500000101080a4330e7ecbf086619474554202f6367692d62696e2f62726f7773652d65646761723f43494b3d5046532646696e643d536561726368266f776e65723d6578636c75646526616374696f6e3d676574636f6d70616e7920485454502f312e310d0a486f73743a207777772e7365632e676f760d0a436f6e6e656374696f6e3a206b6565702d616c6976650d0a4163636570742d456e636f64696e673a20677a69702c206465666c6174650d0a4163636570743a202a2f2a0d0a557365722d4167656e743a20707974686f6e2d72657175657374732f322e392e310d0a0d0a',\n",
       " '90b11c2583303c15c2d18eb6080045000034ac8a4000400673750a30511c6869570ffc520050caafeadbe0c88c4e80100fcedbb700000101080a4330e862bf086691',\n",
       " '90b11c2583303c15c2d18eb6080045000034353f40004006eac00a30511c6869570ffc520050caafeadbe0c8960a80100fb2d21700000101080a4330e862bf086691',\n",
       " '90b11c2583303c15c2d18eb6080045000034a3ca400040067c350a30511c6869570ffc520050caafeadbe0c8960a80111000d1c700000101080a4330e863bf086691',\n",
       " '90b11c2583303c15c2d18eb6080045000034756e40004006aa910a30511c6869570ffc520050caafeadce0c8960b80101000d10d00000101080a4330e8bebf0866ef']"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badSessionDict['session_11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hexSessions['session_11']==badSessionDict['session_11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:  0a30511c ,  dest:  6869570f\n",
      "source:  0a30511c ,  dest:  6869570f\n",
      "source:  0a30511c ,  dest:  6869570f\n",
      "source:  0a30511c ,  dest:  6869570f\n",
      "source:  0a30511c ,  dest:  6869570f\n",
      "source:  0a30511c ,  dest:  6869570f\n",
      "source:  0a30511c ,  dest:  6869570f\n"
     ]
    }
   ],
   "source": [
    "for packet in hexSessions['session_11']:\n",
    "    print 'source: ', packet[52:60], ', ', 'dest: ', packet[60:68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['90b11c2583303c15c2d18eb6080045000040e00b400040063fe80a30511c6869570ffc520050caafea0500000000b002ffff401f0000020405b4010303050101080a4330e7900000000004020000',\n",
       " '90b11c2583303c15c2d18eb608004500003493d3400040068c2c0a30511c6869570ffc520050caafea06e0c8848e8010100ce4fc00000101080a4330e7ecbf086619',\n",
       " '90b11c2583303c15c2d18eb60800450001090e5d4000400610ce0a30511c6869570ffc520050caafea06e0c8848e8018100c6eb500000101080a4330e7ecbf086619474554202f6367692d62696e2f62726f7773652d65646761723f43494b3d5046532646696e643d536561726368266f776e65723d6578636c75646526616374696f6e3d676574636f6d70616e7920485454502f312e310d0a486f73743a207777772e7365632e676f760d0a436f6e6e656374696f6e3a206b6565702d616c6976650d0a4163636570742d456e636f64696e673a20677a69702c206465666c6174650d0a4163636570743a202a2f2a0d0a557365722d4167656e743a20707974686f6e2d72657175657374732f322e392e310d0a0d0a',\n",
       " '90b11c2583303c15c2d18eb6080045000034ac8a4000400673750a30511c6869570ffc520050caafeadbe0c88c4e80100fcedbb700000101080a4330e862bf086691',\n",
       " '90b11c2583303c15c2d18eb6080045000034353f40004006eac00a30511c6869570ffc520050caafeadbe0c8960a80100fb2d21700000101080a4330e862bf086691',\n",
       " '90b11c2583303c15c2d18eb6080045000034a3ca400040067c350a30511c6869570ffc520050caafeadbe0c8960a80111000d1c700000101080a4330e863bf086691',\n",
       " '90b11c2583303c15c2d18eb6080045000034756e40004006aa910a30511c6869570ffc520050caafeadce0c8960b80101000d10d00000101080a4330e8bebf0866ef']"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hexSessions['session_11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0a30511c'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#destination ip\n",
    "binascii.hexlify(str(pcaps[9]))[60:68]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6869570f'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorce ip [int(x, 16) for x in bytes] then do a join with '.'\n",
    "binascii.hexlify(str(pcaps[9]))[52:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'0a'.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 48 81 28\n"
     ]
    }
   ],
   "source": [
    "print int('0a', 16), int('30', 16), int('51', 16), int('1c', 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Ether  dst=3c:15:c2:d1:8e:b6 src=90:b1:1c:25:83:30 type=0x800 |<IP  version=4L ihl=5L tos=0x0 len=1282 id=43381 flags= frag=0L ttl=59 proto=tcp chksum=0xb6bc src=104.105.87.15 dst=10.48.81.28 options=[] |<TCP  sport=http dport=64573 seq=3777205024 ack=268270148 dataofs=8L reserved=0L flags=PA window=939 chksum=0x8097 urgptr=0 options=[('NOP', None), ('NOP', None), ('Timestamp', (3204984245, 1127263843))] |<Raw  load='\\x8aN\\xe8\\xf4\\xbaSpIZu\\x91|\"C\\\\\\x08d\\xda\\xf0\\xf1m\\x9e9F\\t\\xe8k\\x13\\xfck\\x94\\x80M\\xdf\\x8e\\xd3\\xcf\\xeb\\xf1-\\xe3\\x81\\x01B\\x9bH\\xd0\\xc7\\xae\\x85\\x03\\xa9\\xcb+\\xb9\\xcd|~\\x80\\xd4\\xba\\x82\\x16\\xa9\\xf6\\x00\\xa7\\x80\\x05\\x86`I[\\x8d\\x0f\\xc2b\\xc2\\xd3\\xa66\\x91$\\x9b;\\x04\\x0b\\x94\\xa0\\xb3\\x87\\x85\\x0bK\\xe1\\x1b\\x95$\\xf1y,O\\x87\\xb4\\x03tH\\x9b\\xdd\\x1c\\xbeM,\\xef\\xf3X\\xe7,C\\xe6x\\xe5\\xdc\\xdd#\\x9d\\xfdy,B\\x9a\\xf4\\x8f^\\xd4\\xe27\\x00\\x00\\x00\\xff\\xff\\xd4\\\\\\xcbj\\xc30\\x10\\xfc\\x95\\x1eSH@\\xda\\xd5\\xca\\xd2\\xb5\\xf4\\x03z\\xef\\xad\\xe7BKK\\xff\\xbf+9\\xce\\x0b\\xa1H\\xf6Z\\x91/!\\x88\\x10d<\\x9e\\x9d\\xdd\\x19\\xb9W\\xfe3C\\n\\xe1\\xb2\\x9a~\\xd9\\xdc}\\xdaRnr\\x05\\xaa\\x8a\\n\\xbb\\xcfc1n\\x94v\\xb3\\xa9P\\xae\\xfd;N\\xdd\\x19*\\x03o\\xf0j\\xea\\x1e\\xeeKX\\xec\\xbb\\xfd\\xbb\\xddm\\xb6\\xfd+\\x1e\\x80\\x8e(\\xea}\\x88@Z{\\x18\\xacTZ\\xc2\\x80y!q\\xfeI\\x89u#\\x0b\\xaa\\xb7\\x9f\\xaf\\xdfo.U\\x7f\\xbfO\\xef1\\xba\\xc7W\\xb2\\xfbx\\xdeQA\\xb1:\\x01&\\x91\\x850\\xb68\\x06\\xca\\x80\\t\\x16b\\xa3,\\x84\\xf6\\xfc\\xcc\\xdeG\\xcc\\xf9\\xa7\\x97\\x90\\xb1Z\\x91\\x9aI<k\\x14(L\\x01Dv:\\x90.P!~\\xb5\\x0f\\x9f\\xb0\\x0f\\xc5\\n\\xf7\\x15\\xd5*7\\x00(\\x0e\\xec\\xb5\\x03\\xcd\\xb2Z\\x15 \\x83R,\\xb3\\x02\\x840\\xa5\\xe2q\\x96\\x8a\\x9f\\x05\\xa1J\\xf0`N W\\x82\\xa7M\\x89Z\\x04\\x1e\\xb2v\\xf0\\xfd\\xf0\\r\\xa6\\x041\\xb6\\x10\\xc4\\x01,\\x15(\\xc9\\tbW\\xa3\\x87\\xd5\\x16\\xf4\\xb06\\xe8M\\xcf\\x1c\\x93\\xd21\\xd8`\\xe8X4G\\xc2\\x9c\\xee\\xc5\\xe2\\x894\\xff\\x99\\xef?$\\xcc`\\x01\\x1c\\xdc\\\\N\\x89\\xcd\\xd3\\xab\\x90\\xcd\\xe1Q\\x03i\\x02\\xefO6G\\\\\\n7f\\\\\\\\\\xd3\\xe6\\x18_L\\xf5y>iP\\xd8U%\\xfc\\x8d\\x9b]\\xe7\\xc6\\x92\\x15\\xa2X\\xb9\\x03\\xf4\\x1e#!\\xad\\xac7\\x05\\x92\\xf8a\\xdc\\x03)\\xee\\x81^\\x0c\\x0f\\xc8\\x1a\\x1eT\\x05\\x16\\xad\\xba\\xe7\\x1eeBBd\\t\\xf7\\xc8Z\\xac\\x14\\xbf\\xd3\\x95\\xc5\\x1ao\\x0c/n\\xd2b\\x9d6/a\\xb1\\xb6\\x04\\xd5\"\\x062|\\xc1R>\\xfe\\x1aF\\t\\xa4\\xbat\\x98U\\xdc\\x1a\\x18%\\x80[0J We]y\\x1f\\xc8\\x10W\\xfd{\\x7f\\xdc\\xd0X?<\\x9e7\\xa7\\xacf\\xe4Mw\\x9d\\xd5\\x1c\\xa9\\xc7m3\\xab9m>\\x0b\\xaa\\x1a\\xdel\\x04\\xaaE\\xbc\\x89\\xc4\\xe2M\\x887\\xd7Pn*E\\x9b\\xb2\\xa7\\x8f%\\xdc7\\xc8\\x1d@F]Wo7\\xe0\\xda*~d\\xec\\xfdz\\xdb\\xb0\\x81$g\\x8f\\xafC\\xb8h\\xc5\\xe2\\xe2\\xe6\\x1a\\xc8\\xb8\\xebl\\x08\\xaabz\\xa5\\x86\\x03\\xf4~\\xdc\\x85\\xbcw\\xff\\x00\\x00\\x00\\xff\\xff23\\xa5\\xea\\x92q\\xaa\\'/3CsK\\x8c\\xe4\\x05\\x12\\x1cz\\xc9\\x0b\\xe4j|\\xe3\\x13$\\xa6\\xaeA\\xdf;\\xb0\\xb44\\x01\\xf6\\xf3\\xc8,\\xachP\\xa9\\x19Z`\\xa9\\xd4\\x0c\\xe9p^\\x82\\xa9\\x9e\\x819\\xc1\\xaa\\xcc\\x10\\xdf\\x06NS\\x12V\\x00\\x18\\x98\\xe9\\x1a\\x0e\\xf6\\xc34L-M\\x80\\xed\\xeaA\\xb8\\x00\\x1c\\xb6Y\\x05\\xd2\\xca6C\\xd9\\xac\\x02m\\xa8\\xd2{;\\'\\x00\\x00\\x00\\xff\\xff\\xed\\x9d\\xdfJ\\xc30\\x14\\xc6\\xef}\\x8a\\xb2\\x170i\\x9a\\xfe\\xb9)x#^(\\x93\\xaa\\x0f \\xb4\\x83\\xc1 \\xa2\\xbb\\xf1\\xed=I\\x9a\\x9aj\\x96\\xb5\\xeb\\x99K\\xc0\\xdd\\xacte\\x1c\\x9a\\x8fs\\x9a\\xf4\\xe4\\xf7\\xe1lV1\\xc1\\xe3lV\\x91\\xe8\\xa8\\xf07\\xda\\xf1\\x12J\\xc8\\xc9\\xc4\\x96s,G\\xd0\\xdc\\x95\\x82\\xc2\\xe3\\xe8\\x99\\xc0bX\\x8e\\xa0\\xde-\\xca3\\xdeKK Z\\xf0\\x89\\xb3\\xcc\\n\\x9a\\x87\\xc9K\\xd0\\n\\xcf\\\\\\n\\x0f\\x89\\xa3gB\\xf2\\xae\\xfe\\xcfy\\x16\\xe3\\x11,c\\x95Y\\nO\\xa2\\'f\\xc2\\x9b\\xe6i\\xa9L*\\xf3Q\\xa5\\xb5\\xca\\xab\\xd2:\\xa5\\xc7D\\x9eD/\\xad\\x8f\\xafo\\xdd\\xbb*\\xad=\\xad\\xa5\\xd1\\xa9f/\\x92\\x9eV\\xf4\\x99\\xdc\\x89]\\x0b\\xd5t\\xac\\x18Wt\\xde\\xf6\\x97Yz\\t\\xfeU5!\\x05<, \\xa5\\x19|\\x0c\\xa3N5\\xa9+\\xd5\\xc4\\x88a4\\xa1{\\x04VL\\xc70\\xf2(xz\\xbc\\x80\\xf9\\xe1\\x04\\xd4\\xc6AI!S\\x17\\x95\\xa4\\x88\\xab#\\x8f\\xe0\\xae5,\\xa0.\\x9axp\\xfa\\xf0bQ\\te\\xc7\\xb7\\x9b_\\x82\\xab\\xa7\\x15\\xe3j\\xcbC\\x06\\x1a\\x9c\\x8d\\xabg\\xa2\\xf5\\xf4\\x92Ws*\\x1b\\xfb\\x9b&,\\xc5\\xca#\\x0c\\xe6\\xb6\\xd3\\xb0z\\xeaRKR)\\xa7\\xecHm\\x1b\\x8e\\x07\"\\xbe\\xdb\\x93\\xe8\\xd2\\xf6\\x0e\\xffv\\x05\\xdfc\\xf5\\xcb\\xa0C\\x7f\\xd9.\\x8e\\xb7\\xeb\\xf5\\xf3O\\'\\xd4\\x8d\\x10\\xfbN\\xfbB\\x8d<\\xde\\xf4$\\xe3\\xa5\\xb9_\\xd5\\x0e\\xebF\\xd7\\xbd\\x19$b\\xff\\x8ft\\x9c\\xfdXf\\xa9\\x8a\\xe9\\xfb\\xb8\\xd4cud\\t(\\xda\\xedf\\xdb\\xb5\\xab\\xfa\\xa1?J\\x08\\xbb\\x86\\x19 d\\x83\\xf4\\x80\\xbf\\xa65\\x02\\xf0\\x9bt\\xaf\\x95\\xd7H7\\xc9\\xfa\\xea\\x0b\\xbf\\xef\\xce\\xe2\\x8cx\\x00\\x00' |>>>>"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcaps[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO to make the data richer may want to take out the random parts of the header (e.g. checksumk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#turn the sessions into a dictionary key = session_number, val = list of packages in hex\n",
    "\n",
    "i=0\n",
    "hexSessions = {}\n",
    "\n",
    "for k,v in sessionPrep.items(): # v is the session\n",
    "    #for attr, value in v.__dict__.iteritems(): THIS IS TO GET DICT OF VALUES\n",
    "    #    print attr, value\n",
    "    #if i == 2:\n",
    "    #    break\n",
    "    scpcaps = []    \n",
    "    for p in v: #p is the individual packet in the session\n",
    "        \n",
    "        try:\n",
    "            rawindex = len(p[Raw])\n",
    "            payloadLens.append(rawindex)\n",
    "            scpcaps.append(binascii.hexlify(str(p.original)[:-rawindex])) #turn it into hex\n",
    "        except:\n",
    "            scpcaps.append(binascii.hexlify(p.original))\n",
    "        #for attr, value in p.payload.__dict__.iteritems():#this give the fields that are accessable\n",
    "        #    print attr, value\n",
    "        \n",
    "        #print len(binascii.hexlify(p.original))\n",
    "    hexSessions['session_' + str(i)] = scpcaps\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    ",\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    ",\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    ",\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    ",\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    ",\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    ",\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    ",\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    ",\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    ",\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "hexList = hexstring.lower().split(',')\n",
    "hexList.append('<EOP>') #End Of Packet token\n",
    "hexDict = {}\n",
    "    \n",
    "for key, val in enumerate(hexList):\n",
    "    if len(val) == 1:\n",
    "        val = '0'+val\n",
    "    hexDict[val] = key  #dictionary k=hex, v=int  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Is padding on top (older timesteps/sessions) better than padding on bottom (recent timesteps)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "maxPackets = 10\n",
    "#def hexOneHot(number):\n",
    "#    zeroVec = np.zeros(257)\n",
    "#    zeroVec[number] = 1.0\n",
    "#    return zeroVec\n",
    "\n",
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    "\n",
    "def oneHotSessions(sessionDict, maxPackets = maxPackets, packetTimeSteps = 256,\n",
    "                   reverse = False, charLevel = False):\n",
    "    \"\"\"\n",
    "    This takes a list of int tokens and onehot encodes them, pads sessions with zero tensors according to maxPackets\n",
    "    and packet according to packetTimeSteps\n",
    "    \n",
    "    sessionDict = dict of lists of key = sessions and value = list of packets\n",
    "    timeSteps = maximum len of packet. it will be padded with zero vectors is packet is too short.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    listOsessions = []\n",
    "\n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    sessionKeys = sessionDict.keys()\n",
    "    \n",
    "    for session in sessionKeys:\n",
    "        #sessionTensor = np.zeros((maxPackets, packetTimeSteps, vecLen))\n",
    "        #lenSession = len(session)\n",
    "        sessionCollect = []\n",
    "        \n",
    "        if len(sessionDict[session]) > maxPackets: #crop the number of sessions to maxPackets\n",
    "            sessionList = sessionDict[session][:maxPackets]\n",
    "        else:\n",
    "            sessionList = sessionDict[session]\n",
    "        \n",
    "        for packet in sessionList:\n",
    "            packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "            \n",
    "            if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "                packet = packet[:packetTimeSteps-1]\n",
    "            \n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        \n",
    "            pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "            pacMatLen = len(pacMat)\n",
    "        \n",
    "            #padding packet\n",
    "            if reverse:\n",
    "                pacMat = pacMat[::-1]\n",
    "            \n",
    "            if pacMatLen < packetTimeSteps:\n",
    "                #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "                #padding the packet such that zeros are after the actual info for better translation\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "            if pacMatLen > packetTimeSteps:\n",
    "                pacMat = pacMat[:packetTimeSteps, :]\n",
    "                \n",
    "            sessionCollect.append(pacMat)\n",
    "        \n",
    "        #padding session\n",
    "        sessionCollect = np.asarray(sessionCollect)\n",
    "        numPacketsInSession = np.asarray(sessionCollect).shape[0]\n",
    "        if numPacketsInSession < maxPackets:\n",
    "            #pad sessions to fit the \n",
    "            sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession, \n",
    "                                                                 packetTimeSteps, vecLen))) )\n",
    "            \n",
    "        listOsessions.append(sessionCollect)\n",
    "        \n",
    "    return listOsessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sessions = oneHotSessions(hexSessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes output by shifting inputs down in time one step and then copying the last time step to the end.\n",
    "#def targetModifier(targetArray):\n",
    "#    newTarget = np.vstack((targetArray[1:, :], targetArray[-1,:]))\n",
    "#    return newTarget\n",
    "\n",
    "#def targetMaker(listOinputs):\n",
    "    #TODO: do this with arrays\n",
    "#    outputs = []\n",
    "#    for inp in listOinputs:\n",
    "#        outputs.append(targetModifier(inp))\n",
    "#    outputs = np.asarray(outputs)\n",
    "#    \n",
    "#    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim = 257 #original data dimension/timesteps/columns\n",
    "rnnType = 'gru' #gru or lstm\n",
    "bidirectional = True\n",
    "X = T.tensor3('inputs')\n",
    "Xrev = T.matrix('reversed_inputs')\n",
    "linewt_init = Uniform(width=0.08)\n",
    "rnnwt_init = IsotropicGaussian(0.05)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "###RECURRENT LAYER\n",
    "\n",
    "#To use or not to use that is the question\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data1, data2 = fork.apply(X)\n",
    "\n",
    "###for raw inputs\n",
    "#data1 = X\n",
    "#data2 = T.concatenate([X]*dimMultiplier, axis=2)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hEnc = rnn.apply(data1, data2)[:,-1] #the [:,-1] gets the last hidden state for each obs in minibatch\n",
    "                                         #i.e. the last state for each sentence\n",
    "else:\n",
    "    hinit, _ = rnn.apply(data2)\n",
    "    hEnc = hinit[:,-1]\n",
    "\n",
    "hEnc = T.reshape(hEnc,(maxPackets, 1, dim))\n",
    "#get weights initialized. without weights are nans.\n",
    "fork.initialize()\n",
    "rnn.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST Encoder will return a maxPackets x packet length matrix\n",
    "encoder = theano.function([X], hEnc, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.06395856,  0.16239826,  0.14149277, ..., -0.06800094,\n",
       "         -0.02437934,  0.01854873]],\n",
       "\n",
       "       [[ 0.09869645,  0.23401596,  0.18918398, ..., -0.06635828,\n",
       "         -0.00852821,  0.04297363]],\n",
       "\n",
       "       [[-0.02185388,  0.40972933,  0.37978864, ..., -0.08496918,\n",
       "         -0.01146696, -0.09568208]],\n",
       "\n",
       "       ..., \n",
       "       [[ 0.40630332,  0.56707549,  0.39305404, ..., -0.48502469,\n",
       "         -0.26749712,  0.14190978]],\n",
       "\n",
       "       [[ 0.47011858,  0.60803413,  0.42161939, ..., -0.56104362,\n",
       "         -0.33564985,  0.14849283]],\n",
       "\n",
       "       [[ 0.53092813,  0.6495654 ,  0.45121372, ..., -0.63313448,\n",
       "         -0.40391356,  0.15296829]]], dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST ENCODED PACKETS shape = (maxPackets, 1, dim)\n",
    "encoder(sessions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruContext')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'lstmContext')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "#is encoding at each layer really the best way? or just feeding the raw through?\n",
    "###RECURRENT LAYER\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "            name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data3, data4 = forkContext.apply(hEnc)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hContext = rnnContext.apply(data3, data4)\n",
    "else:\n",
    "    hinitContext, _ = rnnContext.apply(data4)\n",
    "    hContext = hinitContext\n",
    "\n",
    "#THINK ABOUT ADDING L2 POOLING BEFORE CAT\n",
    "if bidirectional:\n",
    "    \n",
    "    data3 = data3[::-1]\n",
    "    data4 = data4[::-1]\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                       biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "        hContextRev = rnnContextRev.apply(data3, data4)\n",
    "    else:\n",
    "        rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                             name = 'lstmContextRev')\n",
    "        hinitContext, _ = rnnContextRev.apply(data4)\n",
    "        hContextRev = hinitContext\n",
    "    \n",
    "    \n",
    "    hContext = T.concatenate((hContext, hContextRev), axis=2)\n",
    "    rnnContextRev.initialize()\n",
    "    \n",
    "#get weights initialized. without weights are nans.\n",
    "forkContext.initialize()\n",
    "rnnContext.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST output shape = (maxPackets, 1, dim*2)\n",
    "context = theano.function([X], hContext, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "context(sessions[1]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#does the fork encoding need to happen here?\n",
    "#do we simply cat the hContext with the next words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimDec = dim*2\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                            biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnDec = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dim, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "\n",
    "data5, data6 = forkDec.apply(hContext)#reduce dimension of bidirectLSTM\n",
    "\n",
    "#decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "#and the last hidden state of the context RNN.\n",
    "data7 = T.concatenate((data5[:-1,:,:], data1[1:, :-1, :]), axis=1) #data1 is the original embedding of X\n",
    "\n",
    "#data8 = T.concatenate((data7, data5), axis = 2)\n",
    "data8, data9 = forkFinal.apply(data7)\n",
    "\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hDec = rnnDec.apply(data8, data9) \n",
    "else:\n",
    "    hinit, _ = rnnDec.apply(data9)\n",
    "    hDec = hinit\n",
    "\n",
    "#Smooth out the probabilities of hDec\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(hDec, extra_ndim = 1)\n",
    "    \n",
    "\n",
    "precost = X[1:, :, :]*np.log(softout) + (1-X[1:, :, :])*np.log(1-softout)\n",
    "cost = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "#cost = BinaryCrossEntropy().apply(X[1:, :, :], softout)\n",
    "\n",
    "#get weights initialized\n",
    "forkDec.initialize()\n",
    "forkFinal.initialize()\n",
    "rnnDec.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "decoder = theano.function([X], cost, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TEST\n",
    "decoder(sessions[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pYx = 1/(1+T.exp(-hDec))\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(pYx, extra_ndim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#test\n",
    "decoderTest = theano.function([X], softout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cost = BinaryCrossEntropy().apply(Y, softout)\n",
    "cg = ComputationGraph([cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "#updates = Adam(params, cost, learning_rate, c=10) #c is gradient clipping parameter\n",
    "updates = RMSprop(cost, params, learning_rate, c=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gradients = T.grad(cost, params)\n",
    "gradients = clip_norms(gradients, 1)\n",
    "gradientFun = theano.function([X], gradients, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling training function\n"
     ]
    }
   ],
   "source": [
    "print \"compiling training function\"\n",
    "train = theano.function([X], cost, updates = updates, allow_input_downcast=True)\n",
    "#predict = theano.function([X], softout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Epoch:  0\n",
      "Epoch cost average:  534.858\n",
      " \n",
      "Epoch:  2\n",
      "Epoch cost average:  533.112\n",
      " \n",
      "Epoch:  4\n",
      "Epoch cost average:  531.842\n",
      " \n",
      "Epoch:  6\n",
      "Epoch cost average:  531.028\n",
      " \n",
      "Epoch:  8\n",
      "Epoch cost average:  530.3\n",
      " \n",
      "Epoch:  10\n",
      "Epoch cost average:  529.437\n",
      " \n",
      "Epoch:  12\n",
      "Epoch cost average:  528.566\n",
      " \n",
      "Epoch:  14\n",
      "Epoch cost average:  527.672\n",
      " \n",
      "Epoch:  16\n",
      "Epoch cost average:  526.717\n",
      " \n",
      "Epoch:  18\n",
      "Epoch cost average:  525.809\n",
      " \n",
      "Epoch:  20\n",
      "Epoch cost average:  524.923\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-178-65290f42e244>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msession\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrainIndex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mcostfun\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mcostCollect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcostfun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    860\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    949\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    950\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 951\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    938\u001b[0m                         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 940\u001b[1;33m                         self, node)\n\u001b[0m\u001b[0;32m    941\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    942\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtheano/scan_module/scan_perform.pyx\u001b[0m in \u001b[0;36mtheano.scan_module.scan_perform.perform (/root/.theano/compiledir_Linux-4.2--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.11-64/scan_perform/mod.cpp:4193)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/opt/conda/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 907\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "random.shuffle(sessions)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(sessions)*trainPercent)\n",
    "\n",
    "#trainData = sessions[0:trainIndex]\n",
    "#testData = sessions[trainIndex:]\n",
    "\n",
    "runname = 'hred'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 200000\n",
    "batch_size = 64\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    costCollect = []\n",
    "\n",
    "    for session in sessions[0:trainIndex]:\n",
    "        costfun = train(session)\n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%2 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        #grads = gradientFun(inputs, outputs)\n",
    "        #for gra in grads:\n",
    "        #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advesarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this deals with making known bad examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hexList = hexSessions['session_11']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n0Q\\x1c'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hexList[0][52:60].decode('hex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paul'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"7061756c\".decode(\"hex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: make a training function\n",
    "runname = 'firstRun'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 200000\n",
    "batch_size = 64\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    \n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, len(trainData),batch_size), range(batch_size, len(trainData), batch_size)):\n",
    "        \n",
    "        inputs = trainData[start:end]\n",
    "        outputs = targetMaker(inputs)\n",
    "        costfun = train(inputs, outputs)\n",
    "        \n",
    "        \n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%30 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        grads = gradientFun(inputs, outputs)\n",
    "        for gra in grads:\n",
    "            print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataPath = '/data/bradspcaps.txt'\n",
    "data = []\n",
    "with open(dataPath, 'rb') as f:\n",
    "    for line in f.readlines():\n",
    "        data.append(line.split(\"'data': \")[-1].split(',')[0].replace(\"'\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Making the hex dictionary\n",
    "hexstring = '0,\t1,\t2,\t3,\t4,\t5,\t6,\t7,\t8,\t9,\tA,\tB,\tC,\tD,\tE,\tF,\t10,\t11,\t12,\t13,\t14,\t15,\t16,\t17,\t18,\t19\\\n",
    ",\t1A,\t1B,\t1C,\t1D,\t1E,\t1F,\t20,\t21,\t22,\t23,\t24,\t25,\t26,\t27,\t28,\t29,\t2A,\t2B,\t2C,\t2D,\t2E,\t2F,\t30,\t31,\t32,\t33,\t34,\t35\\\n",
    ",\t36,\t37,\t38,\t39,\t3A,\t3B,\t3C,\t3D,\t3E,\t3F,\t40,\t41,\t42,\t43,\t44,\t45,\t46,\t47,\t48,\t49,\t4A,\t4B,\t4C,\t4D,\t4E,\t4F,\t50,\t51\\\n",
    ",\t52,\t53,\t54,\t55,\t56,\t57,\t58,\t59,\t5A,\t5B,\t5C,\t5D,\t5E,\t5F,\t60,\t61,\t62,\t63,\t64,\t65,\t66,\t67,\t68,\t69,\t6A,\t6B,\t6C,\t6D\\\n",
    ",\t6E,\t6F,\t70,\t71,\t72,\t73,\t74,\t75,\t76,\t77,\t78,\t79,\t7A,\t7B,\t7C,\t7D,\t7E,\t7F,\t80,\t81,\t82,\t83,\t84,\t85,\t86,\t87,\t88,\t89\\\n",
    ",\t8A,\t8B,\t8C,\t8D,\t8E,\t8F,\t90,\t91,\t92,\t93,\t94,\t95,\t96,\t97,\t98,\t99,\t9A,\t9B,\t9C,\t9D,\t9E,\t9F,\tA0,\tA1,\tA2,\tA3,\tA4,\tA5\\\n",
    ",\tA6,\tA7,\tA8,\tA9,\tAA,\tAB,\tAC,\tAD,\tAE,\tAF,\tB0,\tB1,\tB2,\tB3,\tB4,\tB5,\tB6,\tB7,\tB8,\tB9,\tBA,\tBB,\tBC,\tBD,\tBE,\tBF,\tC0,\tC1\\\n",
    ",\tC2,\tC3,\tC4,\tC5,\tC6,\tC7,\tC8,\tC9,\tCA,\tCB,\tCC,\tCD,\tCE,\tCF,\tD0,\tD1,\tD2,\tD3,\tD4,\tD5,\tD6,\tD7,\tD8,\tD9,\tDA,\tDB,\tDC,\tDD\\\n",
    ",\tDE,\tDF,\tE0,\tE1,\tE2,\tE3,\tE4,\tE5,\tE6,\tE7,\tE8,\tE9,\tEA,\tEB,\tEC,\tED,\tEE,\tEF,\tF0,\tF1,\tF2,\tF3,\tF4,\tF5,\tF6,\tF7,\tF8,\tF9\\\n",
    ",\tFA,\tFB,\tFC,\tFD,\tFE,\tFF'.replace('\\t', '')\n",
    "\n",
    "hexList = hexstring.lower().split(',')\n",
    "hexList.append('EOP') #End Of Packet token\n",
    "hexDict = {}\n",
    "    \n",
    "for key, val in enumerate(hexList):\n",
    "    if len(val) == 1:\n",
    "        val = '0'+val\n",
    "    hexDict[val] = key    \n",
    "\n",
    "#we add 256 on the end to signify the end of the packet ('EOP')\n",
    "tokenizedHeader = [[hexDict[header[i:i+2]] for i in xrange(0,len(header)-2+1,2)]+[256] for header in data]\n",
    "\n",
    "\n",
    "#list of arrays that represent a header with row = time \n",
    "oneHotHeaders = [np.asarray([oneHot(item) for item in header]) for header in tokenizedHeader]\n",
    "\n",
    "normalizedData = normalizeArrays(oneHotHeaders, 253, reverse=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pretraining essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numTokens = 257\n",
    "rnnType = 'gru'\n",
    "X = T.tensor3('inputs')\n",
    "Y = T.tensor3('outputs')\n",
    "linewt_init = Uniform(width=0.02)\n",
    "rnnwt_init = IsotropicGaussian(0.08)\n",
    "rnnbias_init = Constant(0.0)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=numTokens, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnnDec = LSTM(dim=numTokens, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "###ICLR suggestion -> don't use bias in RNNs\n",
    "###RECURRENT LAYER\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=numTokens, output_dims=[numTokens, numTokens * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = rnnbias_init)\n",
    "data5, data6 = forkDec.apply(X)\n",
    "\n",
    "if rnnType == 'gru':\n",
    "    hDec = rnnDec.apply(data5, data6) \n",
    "else:\n",
    "    hinit, _ = rnnDec.apply(data6)\n",
    "    hDec = hinit\n",
    "\n",
    "#CRITICAL: need to loop through the arrays. Do regular people update after every sequence? or minibatch of seqs?\n",
    "    \n",
    "pYx = 1/(1+T.exp(-hDec))\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(pYx, extra_ndim = 1)\n",
    "\n",
    "#get weights initialized\n",
    "forkDec.initialize()\n",
    "rnnDec.initialize()\n",
    "\n",
    "#cost = BinaryCrossEntropy().apply(Y, softout)\n",
    "precost = Y*np.log(softout) + (1-Y)*np.log(1-softout)\n",
    "cost = -T.mean(T.sum(T.sum(precost[:,:-1,:], axis = 2), axis = 1))\n",
    "cg = ComputationGraph([cost])\n",
    "\n",
    "learning_rate = 0.01\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "#updates = Adam(params, cost, learning_rate, c=10) #c is gradient clipping parameter\n",
    "updates = RMSprop(cost, params, learning_rate, c=1)\n",
    "\n",
    "gradients = T.grad(cost, params)\n",
    "gradients = clip_norms(gradients, 1)\n",
    "gradientFun = theano.function([X,Y], gradients, allow_input_downcast=True)\n",
    "train = theano.function([X,Y], cost, updates = updates, allow_input_downcast=True)\n",
    "predict = theano.function([X], softout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ORIGINAL\n",
    "random.shuffle(normalizedData)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(normalizedData)*trainPercent)\n",
    "\n",
    "trainData = normalizedData[0:trainIndex]\n",
    "testData = normalizedData[trainIndex:]\n",
    "\n",
    "runname = 'firstRun'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "\n",
    "epochs = 200000\n",
    "batch_size = 64\n",
    "iteration = 0\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    \n",
    "    costCollect = []\n",
    "\n",
    "    for start, end in zip(range(0, len(trainData),batch_size), range(batch_size, len(trainData), batch_size)):\n",
    "        \n",
    "        inputs = trainData[start:end]\n",
    "        outputs = targetMaker(inputs)\n",
    "        costfun = train(inputs, outputs)\n",
    "        \n",
    "        \n",
    "        costCollect.append(costfun)\n",
    "                \n",
    "        iteration+=1\n",
    "        \n",
    "    ####SAVE COST TO FILE  \n",
    "    if epoch%30 == 0:\n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        grads = gradientFun(inputs, outputs)\n",
    "        for gra in grads:\n",
    "            print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    \n",
    "    np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GPU TO CPU conversion\n",
    "#Now get the weights from the test function. These weights will be numpy arrays\n",
    "w1 = test.get_shared()[0].get_value()\n",
    "\n",
    "#Here the weights are going to be set to the numpy arrays taken from the GPU predict function\n",
    "input_linear.parameters[0].set_value(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.get_shared()[2].get_value().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chars = '1234567890abcdefghijklmnopqrstuvwxyz'\n",
    "words = ['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#we add 256 on the end to signify the end of the packet ('EOP')\n",
    "\n",
    "maxPackets = 10 #limit the number of packets\n",
    "tokSessions = []\n",
    "oneHotSessions = []\n",
    "\n",
    "for ses in hexSessions.keys():    \n",
    "    tokPacket = []\n",
    "    oneHotPacket = []\n",
    "    for p in hexSessions[ses][:maxPackets]:\n",
    "        tokP = [hexDict[p[i:i+2]] for i in xrange(0,len(p)-2+1,2)]+[256] #takes hexstring and tokenizes hex pairs\n",
    "        tokPacket.append(tokP)\n",
    "        oneHotPacket.append(oneHot(tokP))\n",
    "\n",
    "    tokSessions.append(tokPacket)\n",
    "    oneHotSessions.append(oneHotPacket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###ALT RNN LAYER\n",
    "def initialize(to_init):\n",
    "    for bricks in to_init:\n",
    "        bricks.weights_init = initialization.Uniform(width=0.08)\n",
    "        bricks.biases_init = initialization.Constant(0)\n",
    "        bricks.initialize()\n",
    "\n",
    "def gru_layer(dim, h, n):\n",
    "    fork = Fork(output_names=['linear' + str(n), 'gates' + str(n)],\n",
    "                name='fork' + str(n), input_dim=dim, output_dims=[dim, dim * 2])\n",
    "    gru = GatedRecurrent(dim=dim, name='gru' + str(n))\n",
    "    initialize([fork, gru])\n",
    "    linear, gates = fork.apply(h)\n",
    "    return gru.apply(linear, gates)\n",
    "\n",
    "\n",
    "def lstm_layer(dim, h, n):\n",
    "    linear = Linear(input_dim=dim, output_dim=dim * 4, name='linear' + str(n))\n",
    "    lstm = LSTM(dim=dim, name='lstm' + str(n))\n",
    "    initialize([linear, lstm])\n",
    "    return lstm.apply(linear.apply(h))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
