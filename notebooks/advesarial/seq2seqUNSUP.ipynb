{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#how data are represented at each level (forward, backward, forward with padding on top) needs a little\n",
    "    #experimentation to determine the best representation\n",
    "    #also, is encoding at each layer really the best way? or just feeding the raw through?\n",
    "    \n",
    "#Outside web ips are going to be a problem/messy/noisy. Start by categorizing all outside ips by <OUTSIDE_IP>\n",
    "    #instead of the ip address, or another 4 digit symbol to insert into the hex string.\n",
    "    \n",
    "#to help the models generalize more, for a given source ip address with probability p (say p = 0.1) \n",
    "    #use the token <OTHER_MACHINE>\n",
    "    \n",
    "#should we remove random parts of the header, i.e. checksum\n",
    "\n",
    "#should I take out bias for RNNs?\n",
    "\n",
    "#for the decoder,does the fork encoding need to happen ?\n",
    "    #do we simply cat the hContext with the next words?\n",
    "    \n",
    "#Should the architecture just be encode, context and then prediction???\n",
    "\n",
    "#Input data, should it have character and hex pair encoding as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, CuDNN 4007)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['THEANO_FLAGS'] = 'floatX=float32,device=gpu'#,optimizer=fast_compile'\n",
    "\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import cPickle\n",
    "import sys\n",
    "import binascii\n",
    "import multiprocessing as mp\n",
    "from itertools import chain\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "#import ipaddress\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from copy import copy\n",
    "\n",
    "import blocks\n",
    "from blocks.bricks import Linear, Softmax, Softplus, NDimensionalSoftmax, BatchNormalizedMLP,\\\n",
    "                          Rectifier, Logistic, Tanh, MLP\n",
    "from blocks.bricks.recurrent import GatedRecurrent, Fork, LSTM\n",
    "from blocks.initialization import Constant, IsotropicGaussian, Identity, Uniform\n",
    "from blocks.bricks.cost import BinaryCrossEntropy, CategoricalCrossEntropy\n",
    "from blocks.filter import VariableFilter\n",
    "from blocks.roles import PARAMETER\n",
    "from blocks.graph import ComputationGraph\n",
    "\n",
    "import theano\n",
    "from theano import tensor as T\n",
    "\n",
    "\n",
    "module_logger = logging.getLogger(__name__)\n",
    "sys.setrecursionlimit(100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "ts = time.time()\n",
    "timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#start = time.time()\n",
    "dataPath = '/data/fs4/datasets/pcaps/smallFlows.pcap'\n",
    "#pcaps = rdpcap(dataPath)\n",
    "#sessionPrep = pcaps.sessions()\n",
    "#end = time.time()\n",
    "#print end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_header(line):  # pragma: no cover\n",
    "    ret_dict = {}\n",
    "    h = line.split()\n",
    "    if h[2] == 'IP6':\n",
    "        \"\"\"\n",
    "        Conditional formatting based on ethernet type.\n",
    "        IPv4 format: 0.0.0.0.port\n",
    "        IPv6 format (one of many): 0:0:0:0:0:0.port\n",
    "        \"\"\"\n",
    "        ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "        ret_dict['src_ip'] = h[3].split('.')[0]\n",
    "        ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "        ret_dict['dest_ip'] = h[5].split('.')[0]\n",
    "    else:\n",
    "        if len(h[3].split('.')) > 4:\n",
    "            ret_dict['src_port'] = h[3].split('.')[-1]\n",
    "            ret_dict['src_ip'] = '.'.join(h[3].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['src_ip'] = h[3]\n",
    "            ret_dict['src_port'] = ''\n",
    "        if len(h[5].split('.')) > 4:\n",
    "            ret_dict['dest_port'] = h[5].split('.')[-1].split(':')[0]\n",
    "            ret_dict['dest_ip'] = '.'.join(h[5].split('.')[:-1])\n",
    "        else:\n",
    "            ret_dict['dest_ip'] = h[5].split(':')[0]\n",
    "            ret_dict['dest_port'] = ''\n",
    "    return ret_dict\n",
    "\n",
    "\n",
    "def parse_data(line):  # pragma: no cover\n",
    "    ret_str = ''\n",
    "    h, d = line.split(':', 1)\n",
    "    ret_str = d.strip().replace(' ', '')\n",
    "    return ret_str\n",
    "\n",
    "\n",
    "def process_packet(output):  # pragma: no cover\n",
    "    # TODO!! throws away the first packet!\n",
    "    ret_header = {}\n",
    "    ret_dict = {}\n",
    "    ret_data = ''\n",
    "    hasHeader = False\n",
    "    for line in output:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            if not line.startswith('0x'):\n",
    "                # header line\n",
    "                if ret_dict and ret_data:\n",
    "                    # about to start new header, finished with hex\n",
    "                    ret_dict['data'] = ret_data\n",
    "                    yield ret_dict\n",
    "                    ret_dict.clear()\n",
    "                    ret_header.clear()\n",
    "                    ret_data = ''\n",
    "                    hasHeader = False\n",
    "\n",
    "                # parse next header\n",
    "                try:\n",
    "                    ret_header = parse_header(line)\n",
    "                    ret_dict.update(ret_header)\n",
    "                    hasHeader = True\n",
    "                except:\n",
    "                    ret_header.clear()\n",
    "                    ret_dict.clear()\n",
    "                    ret_data = ''\n",
    "                    hasHeader = False\n",
    "\n",
    "            else:\n",
    "                # hex data line\n",
    "                if hasHeader:\n",
    "                    data = parse_data(line)\n",
    "                    ret_data = ret_data + data\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "def is_clean_packet(packet):  # pragma: no cover\n",
    "    \"\"\"\n",
    "    Returns whether or not the parsed packet is valid\n",
    "    or not. Checks that both the src and dest\n",
    "    ports are integers. Checks that src and dest IPs\n",
    "    are valid address formats. Checks that packet data\n",
    "    is hex. Returns True if all tests pass, False otherwise.\n",
    "    \"\"\"\n",
    "    if not packet['src_port'].isdigit(): return False\n",
    "    if not packet['dest_port'].isdigit(): return False\n",
    "\n",
    "    if packet['src_ip'].isalpha(): return False\n",
    "    if packet['dest_ip'].isalpha(): return False\n",
    "\n",
    "    if 'data' in packet:\n",
    "        try:\n",
    "            int(packet['data'], 16)\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def order_keys(hexSessionDict):\n",
    "    \"\"\"\n",
    "    Returns list of the hex sessions in (rough) time order.\n",
    "    \"\"\"\n",
    "    orderedKeys = []\n",
    "\n",
    "    for key in sorted(hexSessionDict.keys(), key=lambda key: hexSessionDict[key][1]):\n",
    "        orderedKeys.append(key)\n",
    "\n",
    "    return orderedKeys\n",
    "\n",
    "\n",
    "def read_pcap(path):  # pragma: no cover\n",
    "    print 'starting reading pcap file'\n",
    "    hex_sessions = {} \n",
    "    proc = subprocess.Popen('tcpdump -nn -tttt -xx -r '+path,\n",
    "                            shell=True,\n",
    "                            stdout=subprocess.PIPE)\n",
    "    insert_num = 0  # keeps track of insertion order into dict\n",
    "    for packet in process_packet(proc.stdout):\n",
    "        if not is_clean_packet(packet):\n",
    "            continue\n",
    "        if 'data' in packet:\n",
    "            key = (packet['src_ip']+\":\"+packet['src_port'], packet['dest_ip']+\":\"+packet['dest_port'])\n",
    "            rev_key = (key[1], key[0])\n",
    "            if key in hex_sessions:\n",
    "                hex_sessions[key][0].append(packet['data'])\n",
    "            elif rev_key in hex_sessions:\n",
    "                hex_sessions[rev_key][0].append(packet['data'])\n",
    "            else:\n",
    "                hex_sessions[key] = ([packet['data']], insert_num)\n",
    "                insert_num += 1\n",
    "\n",
    "    print 'finished reading pcap file'\n",
    "    return hex_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pickleFile(thing2save, file2save2=None, filePath='/work/notebooks/drawModels/', fileName='myModels'):  # pragma: no cover\n",
    "\n",
    "    if file2save2 is None:\n",
    "        f = file(filePath+fileName+'.pickle', 'wb')\n",
    "    else:\n",
    "        f = file(filePath+file2save2, 'wb')\n",
    "\n",
    "    cPickle.dump(thing2save, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def loadFile(filePath):  # pragma: no cover\n",
    "    file2open = file(filePath, 'rb')\n",
    "    loadedFile = cPickle.load(file2open)\n",
    "    file2open.close()\n",
    "    return loadedFile\n",
    "\n",
    "\n",
    "def removeBadSessionizer(hexSessionDict, saveFile=False, dataPath=None, fileName=None):  # pragma: no cover\n",
    "    for ses in hexSessionDict.keys():\n",
    "        paclens = []\n",
    "        for pac in hexSessionDict[ses][0]:\n",
    "            paclens.append(len(pac))\n",
    "        if np.min(paclens)<80:\n",
    "            del hexSessionDict[ses]\n",
    "\n",
    "    if saveFile:\n",
    "        print 'pickling sessions'\n",
    "        pickleFile(hexSessionDict, filePath=dataPath, fileName=fileName)\n",
    "\n",
    "    return hexSessionDict\n",
    "\n",
    "\n",
    "# Making the hex dictionary\n",
    "def hexTokenizer():  # pragma: no cover\n",
    "    hexstring = '''0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, F,\n",
    "                   10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 1A, 1B,\n",
    "                   1C, 1D, 1E, 1F, 20, 21, 22, 23, 24, 25, 26, 27,\n",
    "                   28, 29, 2A, 2B, 2C, 2D, 2E, 2F, 30, 31, 32, 33,\n",
    "                   34, 35, 36, 37, 38, 39, 3A, 3B, 3C, 3D, 3E, 3F,\n",
    "                   40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 4A, 4B,\n",
    "                   4C, 4D, 4E, 4F, 50, 51, 52, 53, 54, 55, 56, 57,\n",
    "                   58, 59, 5A, 5B, 5C, 5D, 5E, 5F, 60, 61, 62, 63,\n",
    "                   64, 65, 66, 67, 68, 69, 6A, 6B, 6C, 6D, 6E, 6F,\n",
    "                   70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 7A, 7B,\n",
    "                   7C, 7D, 7E, 7F, 80, 81, 82, 83, 84, 85, 86, 87,\n",
    "                   88, 89, 8A, 8B, 8C, 8D, 8E, 8F, 90, 91, 92, 93,\n",
    "                   94, 95, 96, 97, 98, 99, 9A, 9B, 9C, 9D, 9E, 9F,\n",
    "                   A0, A1, A2, A3, A4, A5, A6, A7, A8, A9, AA, AB,\n",
    "                   AC, AD, AE, AF, B0, B1, B2, B3, B4, B5, B6, B7,\n",
    "                   B8, B9, BA, BB, BC, BD, BE, BF, C0, C1, C2, C3,\n",
    "                   C4, C5, C6, C7, C8, C9, CA, CB, CC, CD, CE, CF,\n",
    "                   D0, D1, D2, D3, D4, D5, D6, D7, D8, D9, DA, DB,\n",
    "                   DC, DD, DE, DF, E0, E1, E2, E3, E4, E5, E6, E7,\n",
    "                   E8, E9, EA, EB, EC, ED, EE, EF, F0, F1, F2, F3,\n",
    "                   F4, F5, F6, F7, F8, F9, FA, FB, FC, FD, FE, FF'''.replace('\\t', '')\n",
    "\n",
    "    hexList = [x.strip() for x in hexstring.lower().split(',')]\n",
    "    hexList.append('<EOP>')  # End Of Packet token\n",
    "    hexDict = {}\n",
    "\n",
    "    for key, val in enumerate(hexList):\n",
    "        if len(val) == 1:\n",
    "            val = '0'+val\n",
    "        hexDict[val] = key  #dictionary k=hex, v=int  \n",
    "\n",
    "    return hexDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting reading pcap file\n",
      "finished reading pcap file\n"
     ]
    }
   ],
   "source": [
    "hexSessions = read_pcap(dataPath)\n",
    "hexSessions = removeBadSessionizer(hexSessions)\n",
    "hexSessionsKeys = order_keys(hexSessions)\n",
    "hexDict = hexTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary of IP communications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def oneHot(index, granular = 'hex'):\n",
    "    if granular == 'hex':\n",
    "        vecLen = 257\n",
    "    else:\n",
    "        vecLen = 17\n",
    "    \n",
    "    zeroVec = np.zeros(vecLen)\n",
    "    zeroVec[index] = 1.0\n",
    "    \n",
    "    return zeroVec\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def oneSessionEncoder(sessionPackets, hexDict, maxPackets=2, packetTimeSteps=100,\n",
    "                      packetReverse=False, charLevel=False, padOldTimeSteps=True):  # pragma: no cover\n",
    "    \n",
    "    sessionCollect = []\n",
    "    packetCollect = []\n",
    "    \n",
    "    if charLevel:\n",
    "        vecLen = 17\n",
    "    else:\n",
    "        vecLen = 257\n",
    "    \n",
    "    if len(sessionPackets) > maxPackets: #crop the number of sessions to maxPackets\n",
    "        sessionList = copy(sessionPackets[:maxPackets])\n",
    "    else:\n",
    "        sessionList = copy(sessionPackets)\n",
    "\n",
    "    for packet in sessionList:\n",
    "        packet = packet[32:36]+packet[44:46]+packet[46:48]+packet[52:60]+packet[60:68]+\\\n",
    "                 packet[68:70]+packet[70:72]+packet[72:74]\n",
    "        packet = [hexDict[packet[i:i+2]] for i in xrange(0,len(packet)-2+1,2)]\n",
    "            \n",
    "        if len(packet) >= packetTimeSteps: #crop packet to length packetTimeSteps\n",
    "            packet = packet[:packetTimeSteps]\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        else:\n",
    "            packet = packet+[256] #add <EOP> end of packet token\n",
    "        \n",
    "        packetCollect.append(packet)\n",
    "        \n",
    "        pacMat = np.array([oneHot(x) for x in packet]) #one hot encoding of packet into a matrix\n",
    "        pacMatLen = len(pacMat)\n",
    "        \n",
    "        #padding packet\n",
    "        if packetReverse:\n",
    "            pacMat = pacMat[::-1]\n",
    "\n",
    "        if pacMatLen < packetTimeSteps:\n",
    "            #pad by stacking zeros on top of data so that earlier timesteps do not have information\n",
    "            #padding the packet such that zeros are after the actual info for better translation\n",
    "            if padOldTimeSteps:\n",
    "                pacMat = np.vstack( ( np.zeros((packetTimeSteps-pacMatLen,vecLen)), pacMat) ) \n",
    "            else:\n",
    "                pacMat = np.vstack( (pacMat, np.zeros((packetTimeSteps-pacMatLen,vecLen))) ) \n",
    "\n",
    "        if pacMatLen > packetTimeSteps:\n",
    "            pacMat = pacMat[:packetTimeSteps, :]\n",
    "\n",
    "        sessionCollect.append(pacMat)\n",
    "\n",
    "    #padding session\n",
    "    sessionCollect = np.asarray(sessionCollect, dtype=theano.config.floatX)\n",
    "    numPacketsInSession = sessionCollect.shape[0]\n",
    "    if numPacketsInSession < maxPackets:\n",
    "        #pad sessions to fit the\n",
    "        sessionCollect = np.vstack( (sessionCollect,np.zeros((maxPackets-numPacketsInSession,\n",
    "                                                             packetTimeSteps, vecLen))) )\n",
    "    \n",
    "    return sessionCollect, packetCollect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    #if iteration%100 == 0:\n",
    "    #    updates.append((lr, lr*0.93))\n",
    "    #else:\n",
    "    #    updates.append((lr, lr))\n",
    "    \n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def floatX(X):\n",
    "    return np.asarray(X, dtype=theano.config.floatX)\n",
    "\n",
    "def dropout(X, p=0.):\n",
    "    if p != 0:\n",
    "        retain_prob = 1 - p\n",
    "        X = X / retain_prob * srng.binomial(X.shape, p=retain_prob, dtype=theano.config.floatX)\n",
    "    return X\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_norm(g, c, n): \n",
    "    '''n is the norm, c is the threashold, and g is the gradient'''\n",
    "    \n",
    "    if c > 0: \n",
    "        g = T.switch(T.ge(n, c), g*c/n, g) \n",
    "    return g\n",
    "\n",
    "def clip_norms(gs, c):\n",
    "    norm = T.sqrt(sum([T.sum(g**2) for g in gs]))\n",
    "    return [clip_norm(g, c, norm) for g in gs]\n",
    "\n",
    "# Regularizers\n",
    "def max_norm(p, maxnorm = 0.):\n",
    "    if maxnorm > 0:\n",
    "        norms = T.sqrt(T.sum(T.sqr(p), axis=0))\n",
    "        desired = T.clip(norms, 0, maxnorm)\n",
    "        p = p * (desired/ (1e-7 + norms))\n",
    "    return p\n",
    "\n",
    "def gradient_regularize(p, g, l1 = 0., l2 = 0.):\n",
    "    g += p * l2\n",
    "    g += T.sgn(p) * l1\n",
    "    return g\n",
    "\n",
    "def weight_regularize(p, maxnorm = 0.):\n",
    "    p = max_norm(p, maxnorm)\n",
    "    return p\n",
    "\n",
    "def Adam(params, cost, lr=0.0002, b1=0.1, b2=0.001, e=1e-8, l1 = 0., l2 = 0., maxnorm = 0., c = 8):\n",
    "    \n",
    "    updates = []\n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    \n",
    "    i = theano.shared(floatX(0.))\n",
    "    i_t = i + 1.\n",
    "    fix1 = 1. - b1**(i_t)\n",
    "    fix2 = 1. - b2**(i_t)\n",
    "    lr_t = lr * (T.sqrt(fix2) / fix1)\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        m = theano.shared(p.get_value() * 0.)\n",
    "        v = theano.shared(p.get_value() * 0.)\n",
    "        m_t = (b1 * g) + ((1. - b1) * m)\n",
    "        v_t = (b2 * T.sqr(g)) + ((1. - b2) * v)\n",
    "        g_t = m_t / (T.sqrt(v_t) + e)\n",
    "        g_t = gradient_regularize(p, g_t, l1=l1, l2=l2)\n",
    "        p_t = p - (lr_t * g_t)\n",
    "        p_t = weight_regularize(p_t, maxnorm=maxnorm)\n",
    "        \n",
    "        updates.append((m, m_t))\n",
    "        updates.append((v, v_t))\n",
    "        updates.append((p, p_t))\n",
    "    \n",
    "    updates.append((i, i_t))\n",
    "    #if iteration%100 == 0:\n",
    "    #    updates.append((lr, lr*0.93))\n",
    "    #else:\n",
    "    #    updates.append((lr, lr))\n",
    "    \n",
    "    return updates\n",
    "\n",
    "def RMSprop(cost, params, lr = 0.001, l1 = 0., l2 = 0., maxnorm = 0., rho=0.9, epsilon=1e-6, c = 8):\n",
    "    \n",
    "    grads = T.grad(cost, params)\n",
    "    grads = clip_norms(grads, c)\n",
    "    updates = []\n",
    "    \n",
    "    for p, g in zip(params, grads):\n",
    "        g = gradient_regularize(p, g, l1 = l1, l2 = l2)\n",
    "        acc = theano.shared(p.get_value() * 0.)\n",
    "        acc_new = rho * acc + (1 - rho) * g ** 2\n",
    "        updates.append((acc, acc_new))\n",
    "        \n",
    "        updated_p = p - lr * (g / T.sqrt(acc_new + epsilon))\n",
    "        updated_p = weight_regularize(updated_p, maxnorm = maxnorm)\n",
    "        updates.append((p, updated_p))\n",
    "    return updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization for both the unsupervised net and the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = T.tensor4('inputs')\n",
    "Y = T.matrix('targets')\n",
    "\n",
    "wtstd = 0.2\n",
    "dimIn = 257 #hex has 256 characters + the <EOP> character\n",
    "dim = 100 #dimension reduction size\n",
    "rnnType = 'gru' #gru or lstm\n",
    "bidirectional = False\n",
    "linewt_init = IsotropicGaussian(wtstd)\n",
    "line_bias = Constant(1.0)\n",
    "rnnwt_init = IsotropicGaussian(wtstd)\n",
    "rnnbias_init = Constant(0.0)\n",
    "packetReverse = False\n",
    "\n",
    "###ENCODER\n",
    "if rnnType == 'gru':\n",
    "    rnn = GatedRecurrent(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'gru')\n",
    "    dimMultiplier = 2\n",
    "else:\n",
    "    rnn = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstm')\n",
    "    dimMultiplier = 4\n",
    "\n",
    "fork = Fork(output_names=['linear', 'gates'],\n",
    "            name='fork', input_dim=dimIn, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "\n",
    "###CONTEXT\n",
    "if rnnType == 'gru':\n",
    "    rnnContext = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                biases_init = rnnbias_init, name = 'gruContext')\n",
    "else:\n",
    "    rnnContext = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init, \n",
    "                      name = 'lstmContext')\n",
    "\n",
    "forkContext = Fork(output_names=['linearContext', 'gatesContext'],\n",
    "            name='forkContext', input_dim=dim, output_dims=[dim, dim * dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "if bidirectional:\n",
    "    dimDec = dim*2\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        rnnContextRev = GatedRecurrent(dim=dim, weights_init = rnnwt_init, \n",
    "                                       biases_init = rnnbias_init, name = 'gruContextRev')\n",
    "        \n",
    "    else:\n",
    "        rnnContextRev = LSTM(dim=dim, weights_init = rnnwt_init, biases_init = rnnbias_init,\n",
    "                             name = 'lstmContextRev')\n",
    "    \n",
    "    rnnContextRev.initialize()\n",
    "\n",
    "else:\n",
    "    dimDec = dim\n",
    "\n",
    "\n",
    "###DECODER\n",
    "if rnnType == 'gru':\n",
    "    rnnDec = GatedRecurrent(dim=dimIn, weights_init = rnnwt_init, \n",
    "                            biases_init = rnnbias_init, name = 'gruDecoder')\n",
    "else:\n",
    "    rnnDec = LSTM(dim=dimIn, weights_init = rnnwt_init, biases_init = rnnbias_init, name = 'lstmDecoder')\n",
    "\n",
    "\n",
    "forkDec = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkDec', input_dim=dimDec, output_dims=[dim, dim*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "forkFinal = Fork(output_names=['linear', 'gates'],\n",
    "            name='forkFinal', input_dim=dim, output_dims=[dimIn, dimIn*dimMultiplier], \n",
    "            weights_init = linewt_init, biases_init = line_bias)\n",
    "\n",
    "#initialize the weights in all the functions\n",
    "fork.initialize()\n",
    "rnn.initialize()\n",
    "\n",
    "forkContext.initialize()\n",
    "rnnContext.initialize()\n",
    "forkDec.initialize()\n",
    "\n",
    "forkFinal.initialize()\n",
    "rnnDec.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This section can be skipped if you want to go directly to the classifier\n",
    "\n",
    "learning_rate = theano.shared(np.array(0.001, dtype=theano.config.floatX))\n",
    "learning_decay = np.array(0.9, dtype=theano.config.floatX)\n",
    "batch_size = 20\n",
    "maxPackets = 3\n",
    "packetTimeSteps = 16\n",
    "attention = False\n",
    "\n",
    "def onestepEnc(X):\n",
    "        data1, data2 = fork.apply(X) \n",
    "\n",
    "        if rnnType == 'gru':\n",
    "            hEnc = rnn.apply(data1, data2) \n",
    "        else:\n",
    "            hEnc, _ = rnn.apply(data2)\n",
    "\n",
    "        return hEnc, data1\n",
    "\n",
    "[hEnc, data1], _ = theano.scan(onestepEnc, X) #(mini*numPackets, packetLen, 1, hexdictLen)\n",
    "hEncReshape = T.reshape(hEnc[:,-1], (-1, maxPackets, 1, dim)) #[:,-1] takes the last rep for each packet\n",
    "                                                             #(mini, numPackets, 1, dimReduced)\n",
    "def onestepContext(hEncReshape):\n",
    "\n",
    "    data3, data4 = forkContext.apply(hEncReshape)\n",
    "\n",
    "    if rnnType == 'gru':\n",
    "        hContext = rnnContext.apply(data3, data4)\n",
    "    else:\n",
    "        hContext, _ = rnnContext.apply(data4)\n",
    "\n",
    "    return hContext\n",
    "\n",
    "hContext, _ = theano.scan(onestepContext, hEncReshape)\n",
    "\n",
    "if attention:\n",
    "    print 'what are you doing?!?!'\n",
    "else:\n",
    "    data5, _ = forkDec.apply(hContext)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ctxtest = theano.function([X], [data1, hContext], allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testones = np.ones((12,16,1,257))\n",
    "testones[0] = testones[0]+3\n",
    "testones[-1] = testones[-1]-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 16, 1, 100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ctxtest(testones)[0].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 1, 100)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(4, 2, 1, 100)\n",
    "ctxtest(testones)[1][:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         ..., \n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197]],\n",
       "\n",
       "        [[-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         ..., \n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197]]],\n",
       "\n",
       "\n",
       "       [[[-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         ..., \n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197]],\n",
       "\n",
       "        [[-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         ..., \n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197]]],\n",
       "\n",
       "\n",
       "       [[[-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         ..., \n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197]],\n",
       "\n",
       "        [[-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         ..., \n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197]]],\n",
       "\n",
       "\n",
       "       [[[-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         ..., \n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197],\n",
       "         [-3.22383356,  2.83601284, -0.62676251, ..., -2.74221468,\n",
       "           1.89804673, -1.32940197]],\n",
       "\n",
       "        [[ 9.44766712, -2.67202544,  4.25352478, ...,  8.48442936,\n",
       "          -0.79609346,  5.65880394],\n",
       "         [ 9.44766712, -2.67202544,  4.25352478, ...,  8.48442936,\n",
       "          -0.79609346,  5.65880394],\n",
       "         [ 9.44766712, -2.67202544,  4.25352478, ...,  8.48442936,\n",
       "          -0.79609346,  5.65880394],\n",
       "         ..., \n",
       "         [ 9.44766712, -2.67202544,  4.25352478, ...,  8.48442936,\n",
       "          -0.79609346,  5.65880394],\n",
       "         [ 9.44766712, -2.67202544,  4.25352478, ...,  8.48442936,\n",
       "          -0.79609346,  5.65880394],\n",
       "         [ 9.44766712, -2.67202544,  4.25352478, ...,  8.48442936,\n",
       "          -0.79609346,  5.65880394]]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.reshape((4, maxPackets, packetTimeSteps, dim)) = (4, 3, 16, 100)\n",
    "#[:,1:,:-1] = (4, 2, 15, 100)\n",
    "ctxtest(testones)[0].reshape((4, maxPackets, packetTimeSteps, dim))[:,1:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#data1, (batch_size, maxPackets, packetTimeSteps, dim))[:,1:,:-1]),\n",
    "#(12, 16, 1, 100)\n",
    "ctxtest(testones)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 16, 100)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "######START HERE!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Transform hContext then add it to the concatenated zeros stuff\n",
    "\n",
    "#(4, 2, 16, 100)\n",
    "#                         (batch_size, maxPackets-1, 1, dim)\n",
    "(np.concatenate((np.zeros((4,2,1,100)),ctxtest(testones)[0].reshape((4, maxPackets, packetTimeSteps, dim))[:,1:,:-1]), axis = 2) + ctxtest(testones)[1][:,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blocks.bricks.recurrent import BaseRecurrent, recurrent\n",
    "from blocks.bricks.recurrent import SimpleRecurrent\n",
    "from blocks import initialization\n",
    "from blocks.bricks import Identity\n",
    "#from blocks.initialization import Identity\n",
    "\n",
    "#@recurrent\n",
    "rnntest = SimpleRecurrent(\n",
    "          dim=6, activation=Identity(), name='second_recurrent_layer',\n",
    "          weights_init=initialization.Identity())\n",
    "\n",
    "rnntest.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testdat1 = T.tensor3()\n",
    "testdat2 = T.matrix()\n",
    "testout = rnntest.apply(inputs = testdat1, states = testdat2, iterate = False)\n",
    "#testout = rnntest.apply(inputs = testdat1, states = testdat2, iterate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testy = theano.function([testdat1], testout, allow_input_downcast=True)\n",
    "testy = theano.function([testdat1, testdat2], testout, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.55550689,  0.28341385,  0.15724478,  0.98274833,  0.78550856,\n",
       "          0.54037572]],\n",
       "\n",
       "       [[ 0.0051092 ,  0.63481661,  0.94328937,  0.10269005,  0.75664565,\n",
       "          0.17752227]],\n",
       "\n",
       "       [[ 0.50084736,  0.64411131,  0.01379403,  0.99626781,  0.1945023 ,\n",
       "          0.03193511]],\n",
       "\n",
       "       [[ 0.24559894,  0.11226367,  0.5846486 ,  0.10384701,  0.12228761,\n",
       "          0.47314758]]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testx = np.random.rand(4,1,6)\n",
    "testx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.55550694,  1.28341389,  1.1572448 ,  1.98274827,  1.78550863,\n",
       "          1.54037571]],\n",
       "\n",
       "       [[ 1.00510919,  1.63481665,  1.9432894 ,  1.1026901 ,  1.75664568,\n",
       "          1.1775223 ]],\n",
       "\n",
       "       [[ 1.50084734,  1.64411139,  1.01379406,  1.9962678 ,  1.19450235,\n",
       "          1.0319351 ]],\n",
       "\n",
       "       [[ 1.24559891,  1.11226368,  1.58464861,  1.10384703,  1.12228763,\n",
       "          1.47314763]]], dtype=float32)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy(testx, np.ones((1,6), dtype=theano.config.floatX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[  8.,   8.,   8.,   8.,   8.,   8.]],\n",
       "\n",
       "        [[  8.,   8.,   8.,   8.,   8.,   8.]],\n",
       "\n",
       "        [[  8.,   8.,   8.,   8.,   8.,   8.]]],\n",
       "\n",
       "\n",
       "       [[[ 14.,  14.,  14.,  14.,  14.,  14.]],\n",
       "\n",
       "        [[ 14.,  14.,  14.,  14.,  14.,  14.]],\n",
       "\n",
       "        [[ 14.,  14.,  14.,  14.,  14.,  14.]]]], dtype=float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy(np.ones((3,1,6)), np.ones((3,1,6))+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.42494223,  0.88823497,  0.27871415,  0.67081416,  0.47666129,\n",
       "          0.6851989 ],\n",
       "        [ 0.42494223,  0.88823497,  0.27871415,  0.67081416,  0.47666129,\n",
       "          0.6851989 ],\n",
       "        [ 0.42494223,  0.88823497,  0.27871415,  0.67081416,  0.47666129,\n",
       "          0.6851989 ],\n",
       "        [ 0.42494223,  0.88823497,  0.27871415,  0.67081416,  0.47666129,\n",
       "          0.6851989 ],\n",
       "        [ 0.42494223,  0.88823497,  0.27871415,  0.67081416,  0.47666129,\n",
       "          0.6851989 ],\n",
       "        [ 0.42494223,  0.88823497,  0.27871415,  0.67081416,  0.47666129,\n",
       "          0.6851989 ]],\n",
       "\n",
       "       [[ 0.60906661,  1.23047972,  0.96374762,  1.46646965,  1.26869583,\n",
       "          1.2148068 ],\n",
       "        [ 0.60906661,  1.23047972,  0.96374762,  1.46646965,  1.26869583,\n",
       "          1.2148068 ],\n",
       "        [ 0.60906661,  1.23047972,  0.96374762,  1.46646965,  1.26869583,\n",
       "          1.2148068 ],\n",
       "        [ 0.60906661,  1.23047972,  0.96374762,  1.46646965,  1.26869583,\n",
       "          1.2148068 ],\n",
       "        [ 0.60906661,  1.23047972,  0.96374762,  1.46646965,  1.26869583,\n",
       "          1.2148068 ],\n",
       "        [ 0.60906661,  1.23047972,  0.96374762,  1.46646965,  1.26869583,\n",
       "          1.2148068 ]],\n",
       "\n",
       "       [[ 1.47735739,  1.73619962,  1.09535182,  1.95769811,  1.44340515,\n",
       "          1.45082724],\n",
       "        [ 1.47735739,  1.73619962,  1.09535182,  1.95769811,  1.44340515,\n",
       "          1.45082724],\n",
       "        [ 1.47735739,  1.73619962,  1.09535182,  1.95769811,  1.44340515,\n",
       "          1.45082724],\n",
       "        [ 1.47735739,  1.73619962,  1.09535182,  1.95769811,  1.44340515,\n",
       "          1.45082724],\n",
       "        [ 1.47735739,  1.73619962,  1.09535182,  1.95769811,  1.44340515,\n",
       "          1.45082724],\n",
       "        [ 1.47735739,  1.73619962,  1.09535182,  1.95769811,  1.44340515,\n",
       "          1.45082724]],\n",
       "\n",
       "       [[ 1.92809761,  2.07699656,  1.39723992,  2.19178081,  2.00996995,\n",
       "          2.18181133],\n",
       "        [ 1.92809761,  2.07699656,  1.39723992,  2.19178081,  2.00996995,\n",
       "          2.18181133],\n",
       "        [ 1.92809761,  2.07699656,  1.39723992,  2.19178081,  2.00996995,\n",
       "          2.18181133],\n",
       "        [ 1.92809761,  2.07699656,  1.39723992,  2.19178081,  2.00996995,\n",
       "          2.18181133],\n",
       "        [ 1.92809761,  2.07699656,  1.39723992,  2.19178081,  2.00996995,\n",
       "          2.18181133],\n",
       "        [ 1.92809761,  2.07699656,  1.39723992,  2.19178081,  2.00996995,\n",
       "          2.18181133]]], dtype=float32)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testy(np.random.rand(4,1,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #this fork makes data5 same dim as data1 (the orig word embedding)\n",
    "\n",
    "#decoding data needs to be one timestep (next packet in session) ahead, thus data1 we ignore the first packet\n",
    "#and the last hidden state of the context RNN.\n",
    "#THINK about L2 pooling before cat\n",
    "#THINK should we concatenate with X instead of data5\n",
    "#if packetReverse:\n",
    "#    data1 = data1[:,::-1]\n",
    "#do we need data5??\n",
    "\n",
    "data7 = T.concatenate((data5[:,:-1], \n",
    "                       T.reshape(data1, (batch_size, maxPackets, packetTimeSteps, dim))[:,1:,:-1]), \n",
    "                      axis = 2)\n",
    "                      \n",
    "                      #data1 is the original embedding of X, data5 is transformed context output\n",
    "                      #get rid of first packet in data 5\n",
    "                      #get rid of last context vector\n",
    "\n",
    "#predicts all but the first packet, i.e. session[1:]\n",
    "#output.shape = (20, 2, 100, 257), (minibatch, \n",
    "def onestepDec(data7):\n",
    "    \n",
    "    data8, data9 = forkFinal.apply(data7) #forkFinal transforms back to original dimIn\n",
    "    \n",
    "    if rnnType == 'gru':\n",
    "        hDec = rnnDec.apply(data8, data9) \n",
    "    else:\n",
    "        hDec, _ = rnnDec.apply(data9)\n",
    "        #hDec = hinit #hDec shape = (batch_size*(maxPackets-1), packetTimeSteps, 257)\n",
    "    \n",
    "    return hDec\n",
    "\n",
    "\n",
    "hDec, _ = theano.scan(onestepDec, data7)\n",
    "\n",
    "hDecReshape = T.reshape(hDec, (batch_size*(maxPackets-1), packetTimeSteps, dimIn))\n",
    "\n",
    "\n",
    "softmax = NDimensionalSoftmax()\n",
    "softout = softmax.apply(hDecReshape, extra_ndim = 1)\n",
    "predX = T.reshape(T.reshape(X,(batch_size, maxPackets, packetTimeSteps, dimIn))[:,1:,:,:], \n",
    "                  (batch_size*(maxPackets-1), packetTimeSteps, dimIn))\n",
    "\n",
    "precost = predX*T.log(softout) + (1-predX)*T.log(1-softout)\n",
    "precost2 = -T.sum(T.sum(precost, axis = 2), axis = 1)\n",
    "#precost2 = -T.mean(T.sum(T.sum(precost, axis = 2), axis = 1))\n",
    "\n",
    "cost = T.mean(precost2)\n",
    "#cost = T.mean(BinaryCrossEntropy().apply(predX, softout))\n",
    "cg = ComputationGraph([cost])\n",
    "\n",
    "learning_rate = theano.shared(np.array(0.0001, dtype=theano.config.floatX))\n",
    "learning_decay = np.array(0.9, dtype=theano.config.floatX)\n",
    "\n",
    "params = VariableFilter(roles = [PARAMETER])(cg.variables)\n",
    "updates = Adam(params, cost, learning_rate, c=1) #c is gradient clipping parameter\n",
    "#updates = RMSprop(cost, params, learning_rate, c=1)\n",
    "\n",
    "#gradients = T.grad(cost, params)\n",
    "#gradients = clip_norms(gradients, 1)\n",
    "#gradientFun = theano.function([X], gradients, allow_input_downcast=True)\n",
    "\n",
    "print \"compiling you beautiful person\"\n",
    "train = theano.function([X], [cost, hContext], updates = updates, allow_input_downcast=True)\n",
    "predict = theano.function([X], [softout, hContext], allow_input_downcast=True)\n",
    "print \"finished compiling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#randomize data\n",
    "hexSessionsKeys = hexSessions.keys()\n",
    "#random.shuffle(hexSessionsKeys)\n",
    "trainPercent = 0.9\n",
    "trainIndex = int(len(hexSessionsKeys)*trainPercent)\n",
    "\n",
    "padOldTimeSteps = False\n",
    "\n",
    "runname = 'hred'\n",
    "epochCost = []\n",
    "gradNorms = []\n",
    "contextCollect = []\n",
    "\n",
    "epochs = 80\n",
    "iteration = 0\n",
    "\n",
    "\n",
    "\n",
    "for epoch in xrange(epochs):\n",
    "    costCollect = []\n",
    "    \n",
    "    for start, end in zip(range(0, trainIndex,batch_size), range(batch_size, trainIndex, batch_size)):\n",
    "        \n",
    "        trainingSessions = []\n",
    "        \n",
    "        for trainKey in range(start, end):\n",
    "            sessionForEncoding = list(hexSessions[hexSessions.keys()[trainKey]])\n",
    "            \n",
    "            #encode a normal session\n",
    "            #oneHotSes = oneSessionEncoder(sessionForEncoding,hexDict = hexDict, packetReverse=packetReverse, \n",
    "            #                              padOldTimeSteps = padOldTimeSteps, maxPackets = maxPackets,\n",
    "            #                              packetTimeSteps = packetTimeSteps)\n",
    "            #trainingSessions.append(oneHotSes[0])\n",
    "            #trainingTargets.append(normalTarget)\n",
    "            \n",
    "            #encode an abby normal session\n",
    "            \n",
    "            oneHotSes = oneSessionEncoder(sessionForEncoding,\n",
    "                                              hexDict = hexDict,\n",
    "                                              packetReverse=packetReverse, \n",
    "                                              padOldTimeSteps = padOldTimeSteps, \n",
    "                                              maxPackets = maxPackets, \n",
    "                                              packetTimeSteps = packetTimeSteps)\n",
    "            \n",
    "            trainingSessions.append(oneHotSes[0])\n",
    "            \n",
    "        sessionsMinibatch = np.asarray(trainingSessions).reshape((batch_size*maxPackets, packetTimeSteps, 1, dimIn))        \n",
    "    \n",
    "        costfun = train(sessionsMinibatch)\n",
    "        costCollect.append(costfun[0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if iteration == 0:\n",
    "            print 'you are amazing'\n",
    "            \n",
    "        iteration+=1\n",
    "        \n",
    "        #if iteration%80 == 0:\n",
    "        #    learning_rate.set_value(learning_rate.get_value() * learning_decay)\n",
    "        #    print '   learning rate: ', learning_rate.get_value()\n",
    "        \n",
    "    ####SAVE COST TO FILE \n",
    "    \n",
    "    if epoch%2 == 0:\n",
    "                \n",
    "        print(' ')\n",
    "        print 'Epoch: ', epoch\n",
    "        epochCost.append(np.mean(costCollect))\n",
    "        contextCollect.append(costfun[1][:4])\n",
    "        print 'Epoch cost average: ', epochCost[-1]\n",
    "        #grads = gradientFun(inputs, outputs)\n",
    "        #for gra in grads:\n",
    "        #    print '  gradient norms: ', np.linalg.norm(gra)\n",
    "        \n",
    "    #np.savetxt(runname+\"_COST.csv\", epochCost, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Two huge extension:\n",
    "#    1) give encoder to all decoding time steps\n",
    "#    2) give predicted output to next time step as well\n",
    "#    CHECK 3) use bidirectional to use give beginning and end of encoder to decoder\n",
    "\n",
    "\n",
    "#294.692"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.rand(3,4)+np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Epoch:  32\n",
    "#Epoch cost average:  311.735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hexSessions[hexSessions.keys()[23]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tester = oneSessionEncoder(hexSessions[hexSessions.keys()[23]],hexDict = hexDict,\n",
    "                                              packetReverse=packetReverse, \n",
    "                                              padOldTimeSteps = padOldTimeSteps, \n",
    "                                              maxPackets = maxPackets, \n",
    "                                              packetTimeSteps = packetTimeSteps)[0]\n",
    "testercheck = oneSessionEncoder(hexSessions[hexSessions.keys()[23]],hexDict = hexDict,\n",
    "                                              packetReverse=packetReverse, \n",
    "                                              padOldTimeSteps = padOldTimeSteps, \n",
    "                                              maxPackets = maxPackets, \n",
    "                                              packetTimeSteps = packetTimeSteps)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testerplus = []\n",
    "for i in range(20):\n",
    "    testerplus.append(tester)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.asarray(testerplus).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sessionsMinibatch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testout = np.squeeze(costfun[1])[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(testout[0], 'b', testout[19], 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(testout[0]-testout[1])*89989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testout[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(predict(np.asarray(testerplus).reshape((batch_size*maxPackets, packetTimeSteps, 1, dimIn)))[:2], axis = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testercheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
